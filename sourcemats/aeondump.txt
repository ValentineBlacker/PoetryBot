

At some point in the past two million years, give or take half a million, the genus of great apes that would become modern humans crossed a unique threshold. Across unknowable reaches of time, they developed a communication system able to describe not only the world, but the inner lives of its speakers. They ascended — or fell, depending on your preferred metaphor — into language.

The vast bulk of that story is silence. Indeed, darkness and silence are the defining norms of human history. The earliest known writing probably emerged in southern Mesopotamia around 5,000 years ago but, for most of recorded history, reading and writing remained among the most elite human activities the province of monarchs, priests and nobles who reserved for themselves the privilege of lasting words.

Mass literacy is a phenomenon of the past few centuries, and one that has reached the majority of the worlds adult population only within the past 75 years. In 1950, UNESCO estimated that 44 per cent of the people in the world aged 15 and over were illiterate; by 2012, that proportion had reduced to just 16 per cent, despite the trebling of the global population between those dates. However, while the full effects of this revolution continue to unfold, we find ourselves in the throes of another whose statistics are still more accelerated.

In the past few decades, more than six billion mobile phones and two billion internet-connected computers have come into the world. As a result of this, for the first time ever we live not only in an era of mass literacy, but also — thanks to the act of typing onto screens ­— in one of mass participation in written culture.

As a medium, electronic screens possess infinite capacities and instant interconnections, turning words into a new kind of active agent in the world. The 21st century is a truly hypertextual arena (hyper from ancient Greek meaning over, beyond, overmuch, above measure). Digital words are interconnected by active links, as they never have and never could be on the physical page. They are, however, also above measure in their supply, their distribution, and in the stories that they tell.

Just look at the ways in which most of us, every day, use computers, mobile phones, websites, email and social networks. Vast volumes of mixed media surround us, from music to games and videos. Yet almost all of our online actions still begin and end with writing text messages, status updates, typed search queries, comments and responses, screens packed with verbal exchanges and, underpinning it all, countless billions of words.

This sheer quantity is in itself something new. All future histories of modern language will be written from a position of explicit and overwhelming information — a story not of darkness and silence but of data, and of the verbal outpourings of billions of lives. Where once words were written by the literate few on behalf of the many, now every phone and computer user is an author of some kind. And — separated from human voices — the tasks to which typed language, or visual language, is being put are steadily multiplying.

Consider the story of one of the information ages minor icons, the emoticon. In 1982, at Carnegie Mellon University, a group of researchers were using an online bulletin board to discuss the hypothetical fate of a drop of mercury left on the floor of an elevator if its cable snapped. The scenario prompted a humorous response from one participant — WARNING! Because of a recent physics experiment, the leftmost elevator has been contaminated with mercury. There is also some slight fire damage — followed by a note from someone else that, to a casual reader who hadnt been following the thread, this comment might seem alarming (yelling fire in a crowded theatre is bad news... so are jokes on day-old comments).

Participants thus began to suggest symbols that could be added to a post intended as a joke, ranging from per cent signs to ampersands and hashtags. The clear winner came from the computer scientist Scott Fahlman, who proposed a smiley face drawn with three punctuation marks to denote a joke -). Fahlman also typed a matching sad face -( to suggest seriousness, accompanied by the prophetic note that it is probably more economical to mark things that are NOT jokes, given current trends.

Within months, dozens of smiley variants were creeping across the early internet a kind of proto-virality that has led some to label emoticons the first online meme. What Fahlman and his colleagues had also enshrined was a central fact of online communication in an interactive medium, consequences rebound and multiply in unforeseen ways, while miscommunication will often become the rule rather than the exception.

Three decades later, were faced with the logical conclusion of this trend an appeal at the High Court in London last year against the conviction of a man for a message of menacing character on Twitter. In January 2010, Paul Chambers, 28, had tweeted his frustration at the closure of an airport near Doncaster due to snow Crap! Robin Hood Airport is closed. Youve got a week and a bit to get your shit together, otherwise Im blowing the airport sky high!!

Chambers had said he never thought anyone would take his silly joke seriously. And in his judgment on the Twitter joke trial, the Lord Chief Justice said that — despite the omission of a smiley emoticon — the tweet in question did not constitute a credible threat although it purports to address “you”, meaning those responsible for the airport, it was not sent to anyone at the airport or anyone responsible for airport security... the language and punctuation are inconsistent with the writer intending it to be or to be taken as a serious warning.

The phrase a victory for common sense was widely used by supporters of the charged man, such as the comedians Stephen Fry and Al Murray. As the judge also noted, Twitter itself represents no more and no less than conversation without speech an interaction as spontaneous and layered with contingent meanings as face-to-face communication, but possessing the permanence of writing and the reach of broadcasting.

Its an observation that speaks to a central contemporary fact. Our screens are in increasingly direct competition with spoken words themselves — and with traditional conceptions of our relationship with language. Who would have thought, 30 years ago, that a text message of 160 characters or fewer, sent between mobile phones, would become one of the defining communications technologies of the early 21st century; or that one of its natural successors would be a tweet some 20 characters shorter?

Yet this bare textual minimum has proved to be the perfect match to an age of information suffusion a manageable space that conceals as much as it reveals. Small wonder that the average American teenager now sends and receives around 3,000 text messages a month — or that, as the MIT professor Sherry Turkle reports in her book Alone Together (2011), crafting the perfect kind of flirtatious message is so serious a skill that some teens will outsource it to the most eloquent of their peers.

    Almost without our noticing, we weave worlds from these snapshots, until an illusion of unbroken narrative emerges

Its not just texting, of course. In Asia, so-called chat apps are re-enacting many millions of times each day the kind of exchanges that began on bulletin boards in the 1980s, complete not only with animated emoticons but with integrated access to games, online marketplaces, and even video calls. Phone calls, though, are a degree of self-exposure too much for most everyday communications. According to the article On the Death of the Phone Call by Clive Thompson, published in Wired magazine in 2010, the average number of mobile phone calls we make is dropping every year… And our calls are getting shorter in 2005 they averaged three minutes in length; now theyre almost half that. Safe behind our screens, we let type do our talking for us — and leave others to conjure our lives by reading between the lines.

Yet written communication doesnt necessarily mean safer communication. All interactions, be they spoken or written, are to some degree performative a negotiation of roles and references. Onscreen words are a special species of self-presentation — a form of storytelling in which the very idea of us is a fiction crafted letter by letter. Such are our linguistic gifts that a few sentences can conjure the story of a life a status update, an email, a few text messages. Almost without our noticing, we weave worlds from these snapshots, until an illusion of unbroken narrative emerges from a handful of paragraphs.

Behind this illusion lurks another layer of belief that we can control these second selves. Yet, ironically, control is one of the first things our eloquence sacrifices. As authors and politicians have long known, the afterlife of our words belongs to the world — and what it chooses to make of them has little to do with our own assumptions.

In many ways, mass articulacy is a crisis of originality. Something always implicit has become ever more starkly explicit that words and ideas do not belong only to us, but play out without larger currents of human feeling. There is no such thing as a private language. We speak in order to be heard, we write in order to be read. But words also speak through us and, sometimes, are as much a dissolution as an assertion of our identity.

In his essay Writing or, the Pattern Between People (1932), W H Auden touched on the paradoxical relationship between the flow of written words and their ability to satisfy those using them
Since the underlying reason for writing is to bridge the gulf between one person and another, as the sense of loneliness increases, more and more books are written by more and more people, most of them with little or no talent. Forests are cut down, rivers of ink absorbed, but the lust to write is still unsatisfied.

Onscreen, todays torrents of pixels exceed anything Auden could have imagined. Yet the hyper-verbal loneliness he evoked feels peculiarly contemporary. Increasingly, we interweave our actions and our rolling digital accounts of ourselves curators and narrators of our life stories, with a matching move from internal to external monologue. Its a realm of elaborate shows in which status is hugely significant — and one in which articulacy itself risks turning into a game, with attention and impact (retweets, likes) held up as the supreme virtues of self-expression.

Consider the particular phenomenon known as binary or reversible language that now proliferates online. It might sound obscure, but the pairings it entails are central to most modern metrics of measured attention, influence and interconnection to like and to unlike, to favourite and to unfavourite; to follow and unfollow; to friend and unfriend; or simply to click or unclick the onscreen boxes enabling all of the above.

    Ours is the first epoch of the articulate crowd, the smart mob of words and deeds fused into ceaseless feedback

Like the systems of organisation underpinning it, such language promises a clean and quantifiable recasting of self-expression and relationships. At every stage, both you and your audience have precise access to a measure of reception the number of likes a link has received, the number of followers endorsing a tweeter, the items ticked or unticked to populate your profile with a galaxy of preferences.

Whats on offer is a kind of perpetual present, in which everything can always be exactly the way you want it to be (provided you feel one of two ways). Everything can be undone instantly and effortlessly, then done again at will, while the machinery itself can be shut down, logged off or ignored. Like the author oscillating between Ctrl-Y (redo) and Ctrl-Z (undo) on a keyboard, a hundred indecisions, visions and revisions are permitted — if desired — and all will remain unseen. There is no need, ever, for any conversation to end.

Even the most ephemeral online act leaves its mark. Data only accumulates. Little that is online is ever forgotten or erased, while the business of search and social recommendation funnels our words into a perpetual popularity contest. Every act of selection and interconnection is another reinforcement. If you cant find something online, its often because you lack the right words. And theres a deliciously circular logic to all this, whereby whats right means only what displays the best search results — just as what you yourself are like is defined by the boxes youve ticked. Its a grand game with the most glittering prizes of all at stake connection, recognition, self-expression, discovery. The internets countless servers and services are the perfect riposte to history an eternally unfinished collaboration, pooling the words of many millions; a final refuge from darkness.

Theres much to celebrate in this profligate democracy, and its overthrow of articulate monopolies. The self-dramatising ingenuity behind even three letters such as LOL is a testament to our capacity for making the most constricted verbal arenas our own, while to watch events unfold through the fractal lens of social media is a unique contemporary privilege. Ours is the first epoch of the articulate crowd, the smart mob of words and deeds fused into ceaseless feedback.

Yet language is a bewitchment that can overturn itself — and can, like all our creations, convince us there is nothing beyond it. In an era when the gulf between words and world has never been easier to overlook, its essential to keep alive a sense of ourselves as distinct from the cascade of self-expression; to push back against the torrents of articulacy flowing past and through us.

For the philosopher John Gray, writing in The Silence of Animals (2013), the struggle with words and meanings is sometimes simply a distraction
Philosophers will say that humans can never be silent because the mind is made of words. For these half-witted logicians, silence is no more than a word. To overcome language by means of language is obviously impossible. Turning within, you will find only words and images that are parts of yourself. But if you turn outside yourself — to the birds and animals and the quickly changing places where they live — you may hear something beyond words.

Grays dismissal of half-witted logicians might be a sober tonic, yet its something I find extraordinarily hopeful — an exit from the despairing circularity that expects our creations either to damn or to save us. If we cannot speak ourselves into being, we cannot speak ourselves out of being either. We are, in another fine philosophical phrase, condemned to be free. And this freedom is not contingent on eloquence, no matter how desperately we might wish that words alone could negotiate the world on our behalf.

If I had to describe being shy, Id say it was like coming late to a party when everyone else is about three glasses in. All human interaction, if it is to develop from small talk into meaningful conversation, draws on shared knowledge and tacit understandings. But if youre shy, it feels like you just nipped out of the room when they handed out this information. W Compton Leith, a reclusive curator at the British Museum whose book Apologia Diffidentis (1908) is a pioneering anthropology of shy people, wrote that they go through life like persons afflicted with a partial deafness; between them and the happier world there is as it were a crystalline wall which the pleasant low voices of confidence can never traverse.

Shyness has no logic it impinges randomly on certain areas of my life and not others. What for most people is the biggest social fear of all, public speaking, I find fairly easy. Lecturing is a performance that allows me simply to impersonate a normal, working human being. Q&As, however, are another matter there the performance ends and I will be found out. That left-field question from the audience, followed by brain-freeze and a calamitous attempt at an answer that ties itself up in tortured syntax and dissolves into terrifying silence. Though this rarely happens to me in real life, it has occurred often enough to fuel my catastrophising imagination.

The historian Theodore Zeldin once wondered how different the history of the world might seem if you told it, not through the story of war, politics or economics, but through the development of emotions. One way of tackling it might be to write the history of shyness, he mused. Nations may be unable to avoid fighting each other because of the myths and paranoias that separate them shyness is one of the counterparts to these barriers on an individual level. The history of shyness might well make a fascinating research project, but it would be hellishly difficult to write. Shyness is by its nature a subjective, nebulous state that leaves little concrete evidence behind, if only because people are often too uncomfortable with their shyness to speak or write about it.

For Charles Darwin, this odd state of mind was one of the great puzzles in his theory of evolution, for it appeared to offer no benefit to our species. However, in research begun in the 1970s, the Harvard psychologist Jerome Kagan suggested that about 10-15 per cent of infants are born shy. Being easily fearful and less socially responsive, they reacted to mildly stressful situations with a quicker heartbeat and higher blood cortisol levels.

At around the same time, the American animal behaviourist Stephen Suomi, working at an animal centre in Poolesville, Maryland, observed a similar percentage of shyness in monkeys, with the same increased heart rate and rise in blood cortisol. Blood testing, and reassigning shy infant monkeys to outgoing mothers, suggested that this shy trait was hereditary. Suomis work might also have inadvertently pointed to the evolutionary usefulness of shyness. When a hole in the chain-link fencing around the centres primate range gave the monkeys a chance to get out, the shy ones stayed put while the bolder ones escaped, only to be hit by a truck when they tried to cross the road.

    Until a few hundred years ago, life was lived far more in public whole families would eat, sleep and socialise together in the same room

Higher primates are social creatures, hard-wired to want to meet and mate; but there might also be some value in their being cautious and risk-avoiding, traits that might over-evolve into excessive timidity. Neither Kagan nor Suomi suggest that shyness is fixed at birth. They see it as a case study in the rich interplay between nature and nurture. Similarly, for Antonio Damasio, professor of neuroscience at the University of Southern California, shyness is a secondary emotion. Unlike primary emotions such as anger, fear and disgust — where there is a large biological and universally felt component — shyness is tuned by experience, leaving it open to a huge amount of cultural conditioning, historical variation and definitional ambiguity.

If shyness is something that adjusts to different cultural and historical contexts, then it must surely have taken on oppressive new forms with the emergence of modern notions of privacy and private life. Until a few hundred years ago, life was lived far more in public. For example, it was quite normal for people to urinate or defecate in public places. Even in private houses, whole families would eat, sleep and socialise together in the same room. Then, gradually, bodily functions and aggressive language and behaviour were rendered increasingly invisible in polite society, thanks to what the late sociologist Norbert Elias called the civilising process that took place in the Western world from the 16th century onwards. As greater physical and psychological boundaries grew up around individuals, particularly among relative strangers in public, there were more opportunities for awkwardness and embarrassment about when these boundaries should be crossed.

More recently, shyness, like other awkward personality traits, has been seen as an affliction to be treated medically rather than as a temperamental quirk. In 1971, the psychologist Philip Zimbardo conducted the Stanford Prison Experiment, with student volunteers acting as prisoners and guards in a pretend prison in the basement of the Stanford University psychology building. The study had to be stopped a week early because the guards were treating the prisoners so brutally, and many of the inmates had adapted by internalising their subordinate positions and sheepishly obeying their tormentors. Zimbardo began thinking of shy people as incarcerating themselves in a silent prison, in which they also acted as their own guards, setting severe constraints on their speech and behaviour that were self-imposed although they felt involuntary.

In 1972, Zimbardo began conducting the Stanford Shyness Survey, starting with his own students and eventually including more than 10,000 interviewees. The odd thing about Zimbardos work was that it revealed that feeling shy was very common — more than 80 per cent of those interviewed said they had been shy at some point in their lives, and more than 40 per cent said they were currently shy — but that it also pioneered the modern tendency to see shyness as a remediable pathology. Methods of calibrating shyness were developed, such as the Cheek and Buss Shyness Scale (after its Wellesley College researchers Jonathan Cheek and Arnold Buss) in 1981, and the Social Reticence Scale, formulated by the psychologists Warren Jones and Dan Russell in 1982. Extreme shyness was redefined as social anxiety disorder, and drugs such as Seroxat (also known as Paxil), which works like Prozac by increasing the brains levels of serotonin, were developed to treat it. As Christopher Lane argues forcefully in his book Shyness How Normal Behaviour Became a Sickness (2007), this was part of a more general biomedical turn in psychiatry, with its growing consensus that traits once attributed to mavericks, sceptics, or mere introverts are psychiatric disorders that drugs should eliminate.

    A small, self-regarding part of me thinks there is something glib about easy articulacy and social skill

In 1999, noting that the number of people identifying as shy in his survey had risen to 60 per cent, Zimbardo told the British Psychological Society that we were on the cusp of a new ice age of non-communication. Computers, email and the replacement of cashiers and shop assistants by cashpoint machines and automated checkouts were all contributing to what he called an epidemic of shyness as the possibilities for human contact diminished. Shyness, he suggested, was no longer an individual problem; it was now a social disease.

Today Zimbardos prediction of a new ice age created by technology seems wide of the mark. On the contrary, the rise of social networking has made it normal for people to lay bare their private lives without inhibition online, from posting photos of themselves in states of inebriation to updating the world on their changing relationship status, in ways that would have seemed inconceivable a generation ago. The internet, far from cutting us off from each other, has simply provided more fodder for our own eras fascination with emotional authenticity and therapeutic self-expression — a shift in public attitudes towards personal privacy that Eva Illouz, professor of sociology at the Hebrew University in Jerusalem, has called the transformation of the public sphere into an arena for the exposition of private life.

In her recent book Quiet The Power of Introverts in a World That Cant Stop Talking (2012), Susan Cain worries about a world ruled by what she calls the extrovert ideal. This, she suggests, found its most malign expression in the excessive risk-taking of those who brought about the banking crisis of 2008. Much of Quiet consists of telling introverts how wonderful they are how we think more deeply and concentrate better than extroverts, are less bothered about money and status, are more sensitive, moral, altruistic, clear-sighted and persistent. If youre an extrovert, the book probably isnt for you.

Yet introversion is not the same as shyness, as Cain is careful to point out, although the two do often overlap. Introverts are people whose brains are overstimulated when in contact with too many other human beings for too long — in which case I am most definitely a shy introvert. If Im in a noisy group of people for more than about an hour, my brain simply starts to scramble like a computer with a system error, and I end up feeling mentally and physically drained. Introverts such as me need to make frequent strategic withdrawals from social life in order to process and make sense of our experiences.

Shyness is something different a longing for connection with other people which is foiled by fear and awkwardness. The danger in simply accepting it, as Cain urges us to do with introversion, is that shyness can easily turn into a self-fulfilling persona — the pose becomes part of you, like a mask that melds with your face. There is always something we cling to in an unhappy situation that stops us escaping from it. In my case, it is the belief that lots of voluble people do not really listen to each other, that they simply exchange words as though they were pinging them over a tennis net — conducting their social life entirely on its surface. A small, self-regarding part of me thinks there is something glib about easy articulacy and social skill.

    The human brain is the most complex object we know, and the journey from one brain to another is surely the most difficult

My more sensible self realises this is nonsense, and that shyness (or, for that matter, non-shyness) has no inherent meaning. There is nothing specific to shyness that makes you more likely to be a nice person, or a good listener, or a deep thinker. Shyness might have certain accidental compensations — being less susceptible to groupthink and more able to examine the habits and rituals of social life with a certain wry detachment, perhaps. Mostly it is just a pain and a burden.

Yet shyness remains a part of being human, and the world would be a more insipid, less creative place without it. As Cain argues, we live in a culture that values dialogue as an ultimate ideal, an end in itself, unburdening ourselves to each other in ever louder voices without necessarily communicating any better. Shyness reminds us that all human interaction is fraught with ambiguity, and that insecurity and self-doubt are natural, because we are all ultimately inaccessible to one another. The human brain is the most complex object we know, and the journey from one brain to another is surely the most difficult. Every attempt at communication is a leap into the dark, with no guarantee that we will be understood or even heard by anyone else. Given this obdurate fact, a little shyness around each other is understandable.

I have often found myself in a circle of people at a social gathering that has suddenly closed up like a scrum and left me standing outside it, as its constituent parts became animated in conversation, forgot I was there, and absent-mindedly nudged me out of the loop. I have fought all my life this sensation that shyness is a personal affliction that has left me viewing our herd-loving, compulsively communicative species from the edges. Now I am coming to see it more as a collective problem, an inevitable by-product of the thing that separates us from other animals our unique human cargo of self-consciousness. For all our need for intimacy, we ultimately face the world alone and cannot enter another persons life or mind without effort and difficulty. Shyness isnt something that alienates me from everyone else; its the common thread that links us all.
The 16th-century Japanese tea master Sen no Rikyu is said to have ignored his hosts fine Song Dynasty Chinese tea jar until the owner smashed it in despair at his indifference. After the shards had been painstakingly reassembled by the mans friends, Rikyu declared Now, the piece is magnificent. So it went in old Japan when a treasured bowl fell to the floor, one didn't just sigh and reach for the glue. The old item was gone, but its fracture created the opportunity to make a new one.

Smashed ceramics would be stuck back together with a strong adhesive made from lacquer and rice glue, the web of cracks emphasised with coloured lacquer. Sometimes the coating was mixed or sprinkled with powdered silver or gold and polished with silk so that the joins gleamed; a bowl or container repaired in this way would typically be valued more highly than the original. According to Christy Bartlett, a contemporary tea master based in San Francisco, it is this gap between the vanity of pristine appearance and the fractured manifestation of mortal fate which deepens its appeal. The mended object is special precisely because it was worth mending. The repair, like that of an old teddy bear, is a testament to the affection in which the object is held.

A similar principle was at work in the boro garments of the Japanese peasant and artisan classes, stitched together from scraps of cloth at a time when nothing went to waste. In boro clothing, the mends become the object. Some garments, like the fabled ship of Theseus, might eventually be overwhelmed by patches; others were assembled from scraps at the outset. In todays trendy Tokyo markets, the technique risks becoming a mere ethnic pose. But boro was always an aesthetic idea as much as an imposition of hardship.

Although quite different in their social status, boro and the aesthetic of repaired ceramics alike draw on the Japanese tradition of wabi-sabi, a world view that acknowledges transience and imperfection. To mend a pot, one must accept whatever its fracture brings one must aspire to mushin — literally no mind — a state of detachment sought by both artists and warriors. As Bartlett explains in her essay A Tearoom View of Mended Ceramics (2008) Accidental fractures set in motion acts of repair that accept given circumstances and work within them to lead to an ultimately more profound appearance.

Mended ceramics displayed their history — the pattern of fracture disclosing the specific forces and events that caused it. Indeed, earlier this year, a team of French physicists from the Aix-Marseille University demonstrated that the starlike cracks in broken glass plates capture a forensic record of the mechanics of the impact. By reassembling the pieces, that moment is preserved. The stories of how mended Japanese ceramics had been broken in the first place — like that of the jar initially spurned by Rikyu — would be perpetuated by constant retelling. In the tea ceremony these histories of the utensils provide raw materials for the stylised conversational puzzles that the host sets his guests.

For years, I have been patching clothes into a kind of makeshift, barely competent boro. Trousers in particular get colonised by patches that start at the knees and at the holes poked by keys around my pockets, spreading steadily across thighs with increasing disregard for colour matching. Only when patches need patches does the recycling bin beckon. At first I did this as a hangover from student privation. Later it became a token of ecological sensibility. Those changing motives carried implications for my appearance the more defiantly visible the mend, the less it risks looking like mere penny-pinching. Thats a foolishly self-conscious consideration, of course, which is why the Japanese aesthetic of repair is potentially so liberating there is nothing defensive about it.

This feels like rather a new idea in the pragmatic West. But things might be changing. Take, for example, the all-purpose mending putty called Sugru, an adhesive silicone polymer that you can hand-mould to shape and then leave overnight to set into a tough, flexible seal. As its website demonstrates, you can use Sugru for all those domestic repairs that are otherwise all but impossible, from cracked toilet seats to split shoes or the abraded insulation on your MacBook mains lead. (Doesnt it always split where it enters the power brick? And isnt it exorbitantly costly to replace?) Sugru was devised by Jane Ní Dhulchaointigh, an Irish design graduate at the Royal College of Art in London, working with a group of retired industrial chemists. Time magazine pronounced it a top invention of 2010, and it has since acquired an avid following of hackers who relish its potential not just to repair off-the-shelf products, but also to modify them.

    It wasnt so much that things stopped working and then got repaired, but that repair was the means by which they worked at all

Sugru doesnt do its job subtly, which is the point. You can get it in modest white, but fans tend to prefer the bright primary colours, giving their repairs maximal visibility. They present mending not as an unfortunate necessity to be carried out as quietly as possible but as an act worth celebrating.

A similar attitude is found in the burgeoning world of radical knitting. Take the textiles artist Celia Pym, who darns peoples clothes as a way of briefly making contact with strangers. There are no invisible mends here Pym introduces bold new colours and patterns, transforming rather than merely repairing the garments. What Pym and the Sugru crew are asserting is that mending has an aesthetic as well as a practical function. They say that if youre going to mend, you might as well do it openly and beautifully.

Their approaches also reflect another of the aesthetic considerations of Japanese ceramic repairs the notion of asobi, a kind of playful creativity introduced by the 16th-century tea master Furuta Oribe. Repairs that embody this principle tended to be more extrovert, even crude in their lively energy. When larger areas of damage had to be patched using pieces from a different broken object, one might plug the gap using fragments that have a totally different appearance, just as clothes today might be patched with exuberant contrasting colours or patterns. Of course, one can now buy new clothes patched this way — a mannered gesture, perhaps, but one anticipated in the way that Oribe would sometimes deliberately damage utensils so that they were not too perfect. This was less a Zen-like expression of impermanence than an exuberant relish of variety.

Such modern fashion statements aside, repair in the West has tended to be more a matter of grumbling and making do. But occasionally the aesthetic questions have been impossible to avoid. When the painting of an Old Master starts cracking and flaking off, what is the best way to make it good? Should we reverently pick up the flakes of paint and surreptitiously glue them back on again? Is it honest to display a Raphael held together with PVA glue? When Renaissance paint fades or discolours, should we touch it up to retain at least a semblance of what the artist intended, or surrender to wabi-sabi? Its safe to assume that no conservator would ever have countenanced the repair last year of the crumbling 19th-century fresco of Jesus in Zaragoza — Ecco Homo by Elías García Martínez — by an elderly churchgoer with the artistic skills of Mr Bean. But does even a skilled retouching risk much the same hubris?

These questions are difficult because aesthetic considerations pull against concerns about authenticity. Who wants to look at a fresco if only half of it is still on the wall? Victorian conservators were rather cavalier in their solutions, often deciding it was better to have a retouched Old Master than none at all. In an age that would happily render Titians tones more acceptable with muddy brown varnish, that was hardly surprising. But todays conservators mostly recoil at the idea of painting over damage in old works, although they will permit some delicate inpainting that fills cracks without covering any of the original paint. Cosimo Turas Allegorical Figure (c. 1455) in the National Gallery in London was repaired this way in the 1980s. Where damage is extensive, it is now common to apply treatments that prevent further decay but leave the existing damage visible.

Such rarefied instances aside, the prejudice against repair as an embarrassing sign of poverty or thrift is surely a product of the age of consumerism. Mending clothes was once routine for every stratum of society. British aristocrats were unabashed at their elbow patches — in truth more prevention than cure, since they protected shooting jackets from wear caused by the shotgun butt. Everything got mended, and mending was a trade.

What sort of trade? Highly skilled, perhaps, but manual, consigning it to a low status in a culture that has always been shaped by the ancient Greek preference for thinking over doing (this is one way in which the West differs from the East). Over the course of the 19th century, the pure theorist gained ascendancy over the applied scientist (or worse still, the engineer); likewise, the professional engineer could at least pull rank on the maintenance man he was a creator and innovator, not a chap with oily rag and tools. Although central to our relationship with things, writes the historian of technology David Edgerton, maintenance and repair are matters we would rather not think about. Indeed, they are increasingly matters wed rather not even do.

Edgerton explains that, until the mid-20th century, repair was a permanent state of affairs, especially for expensive items such as vehicles, which lived in constant interaction with a workshop. It wasnt so much that things stopped working and then got repaired, but that repair was the means by which they worked at all. Repair might even spawn primary manufacturing industries many early Japanese bicycles were assembled from the spare parts manufactured to fix foreign (mostly British) models.

Its not hard to understand a certain wariness about repair what broke once might break again, after all. But its neglect in recent times surely owes something to an underdeveloped repair aesthetic. Our insistence on perfect appearances, on the constant illusion of newness, applies even to our own bodies surgical repairs are supposed to make our own wear and tear invisible, though they rarely do.

Equally detrimental to a culture of mending is the ever more hermetic nature of technology. DIY fixes become impossible either physically (the unit, like your MacBook lead, is sealed) or technically (you wouldnt know where to start). Either way, the warranty is void the moment you start tinkering. Add that to a climate in which you pay for the service or accessories rather than for the item — inks are pricier than printers, mobile phones are free when you subscribe to a network — and repair lacks feasibility, infrastructure or economic motivation. Breakers yards, which used to seem like places of wonder, have all but vanished; car repair has become both unfashionable and impractical. I gave up repairing computer peripherals years ago when the only person I could find to fix a printer was a crook who lacked the skills for the job but charged me the price of a new one anyway.

Some feel this is going to change — whether because of austerity or increasing ecological concerns about waste and consumption. Martin Conreen, a design lecturer at Goldsmiths College in London, believes that TV cookery programmes will soon be replaced by how to DIY shows, in which repair would surely feature heavily. The hacker culture is nurturing an underground movement of making and modifying that is merging with the crowdsourcing of fixes and bodges — for example, on websites such as ifixit.com, which offers free service manuals and advice for technical devices such as computers, cameras, vehicles and domestic appliances. Alternatively there is fixperts.org, set up by the design lecturer Daniel Charny and Sugrus co-founder, James Carrigan, which documents fixes on film.

The mending mindset has taken to the streets in the international Repair Café movement, where you can get free tools, materials, advice and assistance for mending anything from phones to jumpers. As 3D printers — which can produce one-off objects from cured resin, built up from granular inks, layer by layer — become more accessible, it might become possible to make your own spare parts rather than having to source them, often at some cost, from suppliers (only to discover your model is obsolete). And as fixing becomes cool, theres good reason to hope it will acquire an aesthetic that owes less to a make do and mend mentality of soldiering on, and more to mushin and asobi.
Ive been nursing a gentle obsession with a quartet of bone-white, thumb-sized figurines. I first saw them, lined up in a row, on the cover of Miguel Tamens book Friends of Interpretable Objects (2001). They rested in a pair of open hands, looking toothy, and vital, exuding a cool glimmer, while evoking the long Arctic night and the estranging cold. And yet theyre also tiny and personable, these figurines. Their smooth features beckon you to enfold them in the palm of your hand. Their heads are cocked at mad angles, and their leering eyes and rabid smiles bespeak a secret, conspiratorial sociability.

In his book, Tamen contends that objects — art objects in the first instance, but by extension the many things upon which our fascination fixes, such as shells, stones, stars, milk bottles, leaves, and lamps — take up power and life with us as we incorporate them into societies of friends. The figurines on the book cover present an admirable picture of such a society — antic and charismatic friends, whose secret stories one desires to know.

As it turns out, the figurines are never mentioned in the text. Its likely the books designer chose the image, seizing on this quartet of seemingly interpretable objects without paying much attention to the manuscript itself. The books back cover offers a bit of metadata a caption, which describes the objects as devil figures with a provenance of Tuktoyaktuk in Canadas Northwest Territories. The caption says they were carved from the teeth of a blue whale.

I havent been to the Northwest Territories, and I have never seen a blue whale. But I know that they do not range as far north as Tuktoyaktuk, and I know that they do not have teeth. The objects on the cover of Tamens book were likely made from the teeth of beaked whales — dolphins, orcas, and their kin. Perhaps the teeth belonged to belugas, which are still hunted by the Inuvialuit people of Tuktoyaktuk, a remote village in Canadas Northwest Territories, known in recent years as the northern terminus of one of the wintertime highways in Ice Road Truckers, a reality series for cable TV. Further search turns up a link to the original picture at the image bank Corbis, where the note about blue-whale teeth likely originated; the caption reads A person holds devil figures carved from teeth of a blue whale, Tuktoyaktuk, Northwest Territories, Canada. The photo was taken by Lowell Georgia on 1 August 1980; its ID number is LG002968.

    These teeth thrummed, their ivory timbre sending songs across submarine canyons and ice-hung plains of shingle

While the caption might err in identifying the carvings as the teeth of a blue whale, its not far off in calling them devils. The little ivory characters are examples of tupilaq, a genre of carved critter widespread among the Inuit and other peoples of the far north. The tupilaq that live outside of museum time, outside of gallery time, are evil spirits called into being by a shaman for the purpose of making mischief. They carry curses to rivals and enemies. Made from bone and fur and other materials, the tupilaq are powerful magic — and dangerous for those who wield them, for if discovered, their powers turn back on their users unless an immediate public confession is made. Secrecy and darkness are the native habitat of the tupilaq; they lose their power when exposed to the sociable light.

But Im not interested in scolding Corbis or Lowell Georgia, whose photo marvellously evokes the capricious spirit of the tupilaq for one who never has been so far north. For now, Im interested to note the ways in which collectable objects weave shadows and ambiguities around themselves. The light-skinned hands holding the tupilaq in the photo manifest some degree of control over the carvings, but of a kind that can never be total. Objects arrive webbed in connections, and hoard their most intimate gestures and relations in unreachable treasure-houses. A collected object is a kind of vessel, freighted with an irredeemable record of acts and things, inaccessible worlds of sense and event, a tissue of phenomenal dark matter caught up in times obliterative machinery.

The tupilaq, after all, were made from the teeth of an animal whose warm blood surged against tide and ice. These teeth dragged bleeding prey into the black, and tore banners of bubbles through holes in the ice. These teeth thrummed, their ivory timbre sending songs across submarine canyons and ice-hung plains of shingle. Torn from the reek on the blood-soaked shore, these teeth were plucked and cleaned and polished, and carved into devils meant to breed bad luck on a neighbours lodge, his wives, his weapons. Every gesture, every practice of craft and magic and the secret haunts of commerce, took these teeth into a new domain out of the carbon cycle and into the symbolic.

Nor quite this, in the end — for any straightforward dichotomy between the natural and the cultural, the material and the symbolic, is complicated at every instance by qualities that refuse neat abstraction. Toothed whales use their teeth for communication; a porpoises charismatic smile tells a story; dolphins deploy the acoustic properties of their teeth to issue warnings and threats. Rooted in the jaw, the tooth likely aids a whales perceptual work, its capturing and filtering of sound in the marine environment. Forged in an organismic manufactory, tooled by genes (its symbols all the way down), a tooth takes its place for a time in a network of perception and action catching the piercing resonance of whale song bounding in the deep canyons — testing and metering the shifting temperatures of Arctic air — tearing and gripping the trauma-tautened flesh of smolt salmon.

In any case, the ebb and flow of human symbolic culture into which these severed and carved teeth are plunged is never so bounded and legible as our captions and ethnographic accounts propose. These, too, have their tidal forces, responding to the wax and wane of matter and energy, and the disposition of their effects. There is no final shore of culture upon which these teeth make neat landfall; there is only a long-filtering estuary where tide and sign mingle intimately. Drying in the cold wind, a stranded tooth lies on the sand until it marries beauty in some hunters mind, beginning a new transit through channels of sense and symbol. And no matter how figured it becomes, by spells and song, or metadata and scholarship, the tooth remains exposed to the elements.

In this fraught landscape of intermingled effects lies a further twist in the tupilaqs tale, an intertidal eddy these charismatic ivory carvings are a product of the mid-20th century. Before the advent of an outside market for Inuit art sparked their manufacture, a tupilaq would have been made of perishable and sometimes unspeakable matter (such as the corpses of children), contrived not to represent the spirit but to call it forth. By contrast these friendly, spirited carvings were done at the request of white traders and travellers, who wished to see what the real devils — the ones evoked by the dirty, crumbling effigies — actually looked like. These devils were summoned into being by a collision of cultures. Indeed, the tupilaq have long been popular items in the Inuit art trade, where they fetch their makers a tidy price — commerce weaving another veil of secrecy, behind which the tupilaq themselves dance and hide.

I want to understand how things come to take their place — especially in museums and collections — as embodiments of knowledge, artefacts out of time and nature, provoking curiosity and wonder. How they become objectified. The French philosopher Michel Foucault understood the natural history museum as a kind of republic of objects fixed and ordered in their relations. Of course, those relations change with changing science; yesterdays taxonomic specimens become todays harbingers of climate change. This is not to say that the specimens are not friendly to science, that they cannot help us to tell stories about the world. But I want a museum with the modesty to realise that the objects of its interest do not take their sole, true, or final form beneath its gaze. As seen by science, objects withdraw their auras — burning coronas that connect sense and experience to the deep past — and when the galleries and museums are in ruins, they will expose new banners to times unfolding. The tupilaq are players in a luminous, long-durée ecology — one in which paintings and pelts, sculptures and scarab beetles, clay pots and crania change states and meanings; negotiate mingled dimensions of nature and culture; and become consumed, even as they consume our attention.

While at college, I landed a work-study lab-assistant position at the Field Museum of Natural History in Chicago. I found this an immensely exciting prospect the Field Museum (in particular its dinosaur skeletons and its collection of totem poles) had loomed large in my childhood understanding of science, as did natural history itself, as a way of organising experiences of the world. Of course, it was live animals I wanted to get close to — the more charismatic the better — but, unlike the lives they seemed to lead in classic nature documentaries (the kind of close-up ecology depicted in the pages of National Geographic magazine), animals in nature were thinly distributed, cunningly hidden in the fields and mud-smeared woods near my childhood home. I had relished visits to the Field Museum it seemed a place of dense variety, in contrast to which the wider world — at least the part of it I found growing up in the patterned fields of Central Illinois — seemed diluted, intent on hiding tiny quantities of insight and wonder amid swirling, monotonous tides of grey and green.

My job at the Field Museum was in the Mammals Division, working in the specimen-prep lab. The job took me deep into the backstage world of the curatorial departments, a naphtha-scented realm of dioramas and musty cabinets, entered by way of a cunningly hidden elevator behind the colonnade in the museums enclosed façade. Stepping out from the elevator, one emerged into a long, dim corridor where museological tools such as disused vitrines and dusty signs were deposited. The specimen-prep lab, a warren of bright rooms off the corridor, hosted an array of familiar laboratory gear — a fume hood, a steel dissecting table, lab benches cluttered with Pyrex glassware, pipettes, and sturdy boxes of smooth-textured cardboard.

Dusky Seaside Sparrow specimens at the Field Musuem, Chicago. Declared extinct in 1990, by 1980, six remaining individuals, all males, had been captured to establish a captive breeding program but sadly no females were ever found. They lived out their lives in a Walt Disney World nature reserve called Discovery Island. The last male died in June 1987. Photo by Marc Schlossman/Panos Dusky Seaside Sparrow specimens at the Field Musuem, Chicago. Declared extinct in 1990, by 1980, six remaining individuals, all males, had been captured to establish a captive breeding programme. No females were ever found. The males lived out their lives in a Walt Disney World nature reserve called Discovery Island. The last died in June 1987. Photo by Marc Schlossman/Panos

There were distinctive furnishings as well in particular, the maceration tank, a giant stainless-steel pot on a pedestal, a huge pressure cooker used to boil large specimens down to bones. And, behind an airlock-like set of self-sealing doors, the dermestid room — named for the swarms of beetle grubs that seethed over small skeletons, picking them clean. Outfitted with variously sized glass tanks full of grubs, this room was a secure space, with blowers supplying negative air pressure, and seals around the doors, to ensure no beetles or larvae could escape. Upon leaving the dermestid room, you had to stand in the airlock and brush down your clothes. There was an aroma of putrefaction in the room, but it was faint — you got used to it. The sound, however, was oppressive. The place hummed with a static song of tens of thousands of beetle grubs, hairy and grey, all chewing at sinew and dried muscle.

    The holotype is a heady, almost absurd designation an animal sacrificed to represent a life form in its entirety, its desiccated skin and loose, lacklustre fur or feathers standing as avatar for the flashing, teeming, endlessly various individuals of the species

Our task in the specimen-prep lab was to transform dead animals into data. The products of our work were not the taxidermied simulacra that posed behind glass in the galleries, but study skins and skeletons for the research collection. These were stored out of public view in open-topped archival boxes, which fitted closely together into broad, shallow trays that rested in rank upon rank of shelving, forming a library of the dead. Although to call the specimens dead does not sound quite right. For the specimens had transcended or exceeded death, had passed beyond its dominion by means of a process that arrested, ostensibly in perpetuity, their participation in the carbon cycle, the wheel of disarticulation and recombination, that is life on earth.

The collection was not comprised of equals. Enjoying pride of place among the trays were the holotypes, singled out as exemplars of their species. Set off by their yellow tags, type specimens are often much older than their preserved confreres. In most cases, they document the discovery of a species — although of course theyre rarely discoveries in the strict sense of the term. Instead, theyre symbols of a species scientific acknowledgement, of the moment when a local variant achieves a Latin binomial and a place in a refereed journal. The holotype is a heady, almost absurd designation an animal sacrificed to represent a life form in its entirety, its desiccated skin and loose, lacklustre fur or feathers standing as avatar for the flashing, teeming, endlessly various individuals of the species.

However, in dialogue with the singularity of the holotype, it is the specimens in aggregate that give voice and scheme to variety. Any given tray of specimens both expresses and effaces vital relations among discrete creatures; a tray of voles might have all been collected in the same forest or glacial basin, where they comprised a community of bodies jostling, mating, and competing with one another. Other specimens lying nestled together in a case, by contrast, might never have run across one another in life. Now pristine, beyond birth and death, predation and putrefaction, they offer themselves up as information, an apposition of time and place with diameter of nostril, length of genital vent, and body weight in grammes. All of these data gather to sketch the shapes those vital relations take, the specimens in death comprising a kind of rump parliament of the abstract.

We in the lab, too, were specimens of a type, denominated preparators, the inflection of the neo-Latin -ator conferring our place, substantial albeit subordinate, in a guild hierarchy. However, its the prep that captures my attention now for its reformulation of this mortuary processing, this penultimation, as something prefatory or prior. Through our craft, we preparators erased all traces of the troubling carnal processes by which furtive or formidable animals had been reduced to things. By cutting and pulling, stretching and cleaning, we set the clock back to zero.

I want to recall the procedures, the precise craft methods, by which we made such data from the dead. Many of the specimens we received were roadkill, swept up by highway work crews and game wardens. Often in the morning, we were greeted by a fresh delivery of dead birds and bats — a donation of McCormick Place, the vast, glass-clad conference centre on Chicagos lakeshore, whose dusky windows met windblown migrants with unforgiving solidity. We placed these winged specimens in individually dated bags and tossed them into a freezer for later processing. I was especially taken with road-killed fishers, mustelid carnivores of the northern Midwest. Not found in the southerly woods where I grew up, these dog-sized weasels seemed breathtakingly wild to me — even in their frozen morbidity, their ragged costume of matted fur, blood-filled nostrils, and opalescent eyes.

    The limbs I would unroll from their sleeves of skin and fur down to the joints of the feet; then some cunning work to liberate the delicate leaf of the digital bones from its external glove of paw

The work of preparing a study skin consisted in several discrete steps. Take a kangaroo rat recently arrived from the Brookfield Zoo now carefully thawed, still cool from the refrigerator, it lies on the steel table in a loose, parenthetical curl. After first making a series of measurements — length of body, tail, and foot, weight in grammes — I would write out a tiny tag with the relevant geographical coordinates, date of collection, and all-important accession number, all inscribed with an indelible rapidograph pen. Before arriving in the museum, specimens collected by scientists in the field had already been subjected to a great deal of informational dissection external parasites identified and censused, blood and perhaps other tissue samples collected, and finally, the subject itself euthanised and frozen. But in any case, it was my act of inscription in the prep lab that marked the crucial divide, serving as a scientific sacrament by which the transformation into data was signified and enacted. The dead animal was no longer roadkill, no more the victim of snare, trap, or suffocation; it had been accessioned, its death subsumed in the act of collection.

The transformation, however, was far from complete. Next, I would make an incision from the breastbone to the genitals, taking care not to cut through the abdominal wall. Slipping gloved fingers between skin and fascia, I began tediously to work the intact inner body free from the pelt and out the ventral slit I had made. The limbs I would unroll from their sleeves of skin and fur down to the joints of the feet; then some cunning work to liberate the delicate leaf of the digital bones from its external glove of paw. A great deal of care needed to be taken as well around the facial features, detaching them gently from the skull, trying to avoid leaving a fringe of hair attached near the brows, or a tear in the soft nasal tissues.

Invariably, there were lost pieces of information tiny shreds of pad or fascia that stayed adhered to bone as the rest of the animals exterior was torn loose from its sinew-clad skeleton. Often, the extremities would be cut away at the distal ends of the long bones, the paws left intact to hang heavily from the hollowed-out skins. The tail, too, needed tedious unworking from its sleeve of hide — a slow peeling back, bone by bone, to free the pink-sheathed caudal vertebrae, without turning the skin inside-out. A loose sock, a sketch, a gesture vaguely allusive to animality, the skin would now be filled with cotton-wool and pinned out to dry. Organs plucked free, the sticky, flayed rack of bones would then go into the dermestid room to commune with the beetles.

For all its systematic nature, our work bore a family resemblance to that of the shaman fashioning his tupilaq in the prep room, we transformed ephemeral animal matter into storytelling objects. Accessioned, tagged, and arrayed in their boxes, the skins and skeletons were meant to fix and identify connections that bespoke genetic and geographical variation; they were artefacts of phenotype extended in space and time. Likewise, the tupilaq carving is never complete and self-sufficient, but exists as a node, a kind of handhold in a route tying together predation and consumption, spells and songs, the pulses of beauty and value that push it through markets and museums. The prepared specimen, too, acts as a kind of node, articulating its position by way of triangulated theory and data, drifting along tidal shifts of paradigm and intellectual purpose.

Of course, the texture of that tale of variation shifts through time. Natural history museums have never been monolithic, but have changed with the social and institutional make-up of science and its audience, from early modern cabinets of curiosity, to disreputable dime museums, to neoclassical forums for public edification. The Field Museum is of this last type. Founded in the wake of Chicagos 1892 Columbian Exposition, it expressed the epistemology of natural selection in its post-Victorian incarnation variation unfolding in stately, steady grandeur, an ordered autotelos reified in row upon row of mothballed specimens. The advent and acceptance of Mendelian genetics; the growing influence of biogeography; the discovery of DNA, the rise of systematics, genomics, and computational biology — all of these are so many subtle and shifting re-readings of the collection, stories wrung from desiccated data, spirits evoked by these effigies of natural history.

In its ordered cabinets, the specimen collection superimposed and coordinated two different kinds of space. On the one hand there was the hierarchical logic of the classification scheme specimens disposed throughout in boxes, sliding shelves, and jars according to the taxonomy, from kingdom to class to specific epithet. Intersecting this paradigmatic plane was a geographical dimension evoked virtually, via metadata, with each specimens place of collection tagged and noted. Ideally, these two planes interacted in the museum like a multidimensional slide rule for natural history — one calibrated and operated by teams of expert operators, from lab techs to curators to field scientists.

As museums emerged as research institutions during the 18th and 19th centuries, Georges Cuvier championed the epistemic leverage of these centrally coordinated spaces of natural history. From his seat as professor of animal anatomy at the Muséum national dHistoire naturelle in Paris in the late 1790s, Cuvier argued that the observations of the naturalist in the field are broken and fleeting, compared with the powers of the sedentary savant
The sedentary naturalist, it is true, only knows living beings from distant countries through reported information subject to greater or lesser degrees of error, and through samples which have suffered greater or lesser degrees of damage. The great scenery of nature cannot be experienced by him with the same vivid intensity as it can by those who witness it at first hand ... If the sedentary naturalist does not see nature in action, he can yet survey all her products spread before him. He can compare them with each other as often as is necessary to reach reliable conclusions ... The traveller can only travel one road; it is only really in ones study (cabinet) that one can roam freely through the universe ...

The objects disposed in the cabinets of natural history are varied in their qualities and their uses not only study skins and skeletons, but whole creatures preserved in jars of alcohol, germ plasm samples in cryogenic tubes, and sundry other accessions. For purposes of natural science, specimens represent raw data. Indeed, the term preparation serves to re-raw, so to speak — for, like the tupilaq, the dead animals are already thoroughly cooked. And like the tupilaq, they are doubly mysterious.

All the alien, zooic qualia of an animals life — each living creatures unique contexture of sensations and responses, their breeding successes and failures, their urges bred from fear and separation and prey-drive — all these have been irredeemably eradicated in the natural history collection, left behind at the snap of a snare, rinsed down the steely drain of the prep labs necropsy table. Via the same fraught and tedious techniques, they are reinscribed, transformed into esoteric texts that are unreadable without peculiar stores of training, tacit knowledge, and craft practice. The stories they tell, the truths they were meant to exhibit and enact, are nowhere self-evident. An act of predation subsumes and reincorporates phenomenal animal affordances; the scientific sacraments of collecting and accessioning, by contrast, call forth abstract and motive truths, just as the expertise of the shaman reveals and directs the powers of the tupilaq spirits.

    Relay #70 Panel F/(moth) in relay, reads the journal; below, in what is likely Hoppers hand, the line First actual case of bug being found

Recall that in making the tupilaq, the shaman submits to a special code of secrecy if the effigy is discovered or its influence identified, only his immediate public disclosure of the transgression will keep the spirit from turning back on the shaman and his client. The scientist, perhaps, assumes a similar burden in the cycle of knowledge production and publication. The promise of these acts and the artefacts they produce — monographs, journal articles, research bulletins — is the burden of responsibility laid on the museums scientific community by the collection. Unavoidably, or so it seems, these activities demand further acts of collecting and accessioning, which serve to keep the cycle in motion, the spirits restlessly on the move.

In other kinds of museums, objects on display stand for the dark matter of the collection in storage. They are tokens of riches withheld — the vast stored collection not on display but catalogued and preserved as semiotic insurance in the vault. Natural-history collections (in particular those devoted to ecology and evolution, the paleontological and zoological collections) consist almost entirely of objects categorically unsuitable for display — objects that, like the tupilaq, derive their value and agency from the esoteric and out-of-sight dynamics calling them into being.

Bewitched with thoughts of specimens and effigies, I recall yet another object that serially catches my fancy the first computer bug, an archival curiosity collected during the Mark II computer-science research project led by Howard Aiken at Harvard in the mid-1940s. The Mark series was produced in a research programme driven in large part by the brilliance of the American programming pioneer Grace Hopper; the Mark II was one of its early electromechanical computers, instantiating computational logic in a vast, greasy array of switches, shafts, and clutches. Hopper and her colleagues kept a log of their programmatic experimentation, ordering by date and time a journal of the instructions they fed into the machines and the results returned, with terse notes detailing breakdowns and other machine behaviours. Typical lines read 0830, Started machine and 6th degree polynomial Registration trouble most of morning.

In effect, the logbook, which resides in the Smithsonians Museum of American History in Washington DC, is a record of early computer bugs rendered in precise, empirical terms. And in those records, one bug stands out on page 92, at 1545 on 9/9 1945, an actual moth was fixed to the page with a piece of tape. Relay #70 Panel F/(moth) in relay, reads the journal; below, in what is likely Hoppers hand, the line First actual case of bug being found. In record photographs of the journal book, the moth is desiccated, the tape yellow and wrinkled.

Looking for guidance on the nature of this specimen, I wrote to the evolutionary biologist E O Wilson, whose seat as professor at Harvards Museum of Comparative Zoology offers an ideal vantage from which to survey the status of this first computer bug. I'm looking at an object in the Smithsonian that purports to document the first computer “bug”, I wrote to him in an email. Im interested in the moth as a kind of type specimen ... More broadly, I wonder if the “bug” might make an interesting object to muse upon tensions between technology and the natural world — and Im sure your reflections on such a connection would be invaluable.

    Its clear that engineers working on the Mark II were familiar with this usage; the actual bug was entered into the log as a cheeky aside, a bit of lab humour

A couple of weeks later, Wilson sent me a reply. A quarter century or more ago, he wrote, he had asked Hopper herself about the fate of this national treasure, the first computer bug; she had reported that it had gone to the Naval Museum in Washington. Wilson encouraged me to track it down wherever I might find it, and to get an entomologist to identify the species of moth. And, if in the process you can euchre it out of whoever has it, he concluded, and bring it to Harvard's Museum of Comparative Zoology, you will be a local hero.

I doubt I'll be able to pry the bug loose from its current home, the Smithsonian — and in any case, Im not sure it would be curatorially fitting to do so. For in fact, Hoppers moth is not the first computer bug, nor does it furnish us with the origin of the term. Already in the late-19th century, technicians in Thomas Edisons lab were using the word bug to describe thorny technical problems. From the syntax of Hoppers notes in the journal, its clear that engineers working on the Mark II were familiar with this usage; the actual bug was entered into the log as a cheeky aside, a bit of lab humour. Bug is an ancient word, and its use in specific reference to creeping arthropods dates only from the 17th century, according to the Oxford English Dictionary. Prior to that, the word named the nameless bugbears, monsters and creatures of mystery and shadow.

'First actual case of bug being found'. In 1947, engineers working on the Mark II computer at Harvard University found a moth stuck in one of the components and pasted it into the operational logbook. Photo courtesy Naval Surface Warfare Center/Smithsonian 'First actual case of bug being found'. In 1947, engineers working on the Mark II computer at Harvard University found a moth stuck in one of the components and pasted it into the operational logbook. Photo courtesy Naval Surface Warfare Center/Smithsonian

The moth that blundered into the uncanny machinery of the Mark II, following who knows what tracery of heat and light, was of no particular natural-historical significance. Furtive in its shabby grey fluttering, it would have roused no more than a raised eyebrow from Hopper and her busy colleagues servicing the Mark II — a momentary pause, a dropping of cigarette ash on the console. Only later, upon its post-mortem discovery, was this dead creature turned into data. Now roughly preserved and enshrined in the Smithsonian, the dead insect serves as holotype for the computer bug. Like the tupilaq, computer bugs are ungovernable spirits evoked by a kind of transubstantiation. As the uncanny architecture of the computer unfolded itself in Harvards labs, the bug found its way not only into the machines works but into a new role as an object in our midst — a role that took its place among the objects other histories and meanings, its penumbra of qualities.

This patterned assemblage of purposes, roles, and given characteristics, this accidental and ephemeral fate, I want to call by the name habit. An effigy, an insect, an animals measured, pinned-out pelt — we have our ways of domesticating these objects, of bringing them to ground, fixing them in amber or in print. The precise practices vary with what habits we bring to bear (from science to shamanism) and the collections they inhabit. And here is a clue — for dwelling in the word inhabit is habit itself. What if the habits in question are not ours, but those of the objects themselves?

A habit is not only a way of acting, but also a costume of a kind. Some objects — books, dice, celery stalks, lens caps — have deeply ingrained habits, while others — seashells and stars, perhaps, but also bottlecaps, icicles, and plastic six-pack yokes twirling in the mid-ocean gyre — wear their habits more lightly. And some objects take on the habit of naphtha and indelible ink, of cotton wool and alum, of cabinet drawer and taxonomic order.

The word habit catches for me a sense of the shoddy assortment of qualities that knits an object into the fabric of things, weaving into one whole its social roles, the cultural codes it keys, and its whence-and-whither entanglements with deep time.

    I opened the freezer and was greeted by an earsplitting call a kind of wet stridulation at the far end of hearing, its frequency so high it seemed to bore into the jaw. It was the cry of something alive

In its particular death, the Mark II moth took up its own habit of data, clothing itself in the raiment of anecdote and explanation. But had it seen its selective mission — depositing eggs in the folds of Hoppers wool coat hanging from a hook in a Harvard lab, only then to gently expire in a dusty corner — had it escaped to be snatched by a bat, one like those who smashed into the windows of Chicagos McCormick Place with heedless abandon — in these or any myriad number of alternative cases, its habit would have taken different form and colour. Yet the story of any such habits will in the long run prove as ephemeral as the moths wings, trapped like shattered kites beneath the cellophane.

Grace Hoppers moth became the holotype for the computer bug in an instance of creative misunderstanding, betokening our need for clues, artefacts, and documentation. Its tempting to see the whales teeth in a similar light — the blue whale, after all in its vast placidity and inscrutability offers a tempting totem, a synecdoche of oceanic feeling. And perhaps the moth in the machine furnishes a way to re-nature the computer, to redeem lifes agency in the midst of computation.

Once, at the Field Museum, a specimen managed to doff the peculiar mortal habit of data altogether. In the prep lab one morning, I opened the freezer and was greeted by an earsplitting call a kind of wet stridulation at the far end of hearing, its frequency so high it seemed to bore into the jaw. It was the cry of something alive. I began to dig through the piled-up paper bags of frozen specimens, spilling them in heaps on the tiled floor. Finally isolating the bag from which the plaintive alarm issued, I found a little brown bat — a winged walnut pulsing rapidly with breath, alive but too cold to fly. It had arrived with a shipment of window-broken birds from McCormick Place the previous afternoon, some 12 hours before.

Its telling that I struggled with the question of what to do — after all, the bat still was a specimen. Or was it? It had been collected; it was on the threshold of preparation. And yet its vital determination exceeded the bounds of the relevant rituals. Eventually, my prep-lab colleagues and I reached a suitably scientific-seeming conclusion a bat that survived the night in our freezer was a bat that ought to have another chance at the gene pool. And so we found a window overlooking Lake Shore Drive, cracked loose its long-disused latch, and held the bag open in the wind. After a long moment, the bat fled in a blur, disappearing into Chicagos booming late-autumn breeze. It disappeared into the invisible cabinet of its unmeasured curiosity, its habit secreted in the wind.
Every time you set foot in a Whole Foods store, you are stepping into one of the most carefully designed consumer experiences on the planet. Produce is stacked into black bins in order to accentuate its colour and freshness. Sale items peek out from custom-made crates, distressed to look as though theyve just fallen off a farmers truck. Every detail in the store, from the font on a sign to a countertops wood finish, is designed to make you feel like youre in a country market. Most of us take these faux-bucolic flourishes for granted, but shopping wasnt always this way.

George Gilmans early A&P stores are the spiritual ancestors of the Whole Foods experience. If you were a native of small-town America in the 1860s, walking into one of Gilmans A&P stores was a serious culture shock. You would have stared agog at gaslit signage, advertising, tea in branded packages, and a cashier's station shaped like a Chinese pagoda. You would have been forced to wrap your head around the idea of mail-order purchases.

Before Gilman, pre-industrial consumption was largely the unscripted consequence of localised, small-scale patterns of production. With the advent of A&P stores, consumerism began its 150-year journey from real farmers markets in small towns to fake farmers markets inside metropolitan grocery stores. Through the course of that journey, retailing would discover its natural psychological purpose transforming the output of industrial-scale production into the human-scale experience we call shopping.

Gilman anticipated, by some 30 years, the fundamental contours of industrial-age selling. Both the high-end faux-naturalism of Whole Foods and the budget industrial starkness of Costco have their origins in the original A&P retail experience. The modern system of retail pioneered by Gilman — distant large-scale production facilities coupled with local human-scale consumption environments — was the first piece of what Ive come to think of as the American cloud the vast industrial back end of our lives that we access via a theatre of manufactured experiences. If distant tea and coffee plantations were the first modern clouds, A&P stores and mail-order catalogues were the first browsers and apps.

Down at the faux-farm market. Photo by Rebecca Cook/Reuters Down at the faux-farmers market. Photo by Rebecca Cook/Reuters

In the summer of 2011, I found myself in Omaha, Nebraska, a major gateway to the American cloud, having a surreal conversation over lunch with Gary, a software engineer, and Harpreet, a topologist.

Theirs is a calling I could not make up if I tried maintaining Smalltalk applications for Northern Natural Gas, which operates a sprawling 14,000-mile network of pipelines across the Midwest. Smalltalk, something of an indie darling among programming languages, is to mainstream languages such as Java what J R R Tolkiens Elvish is to English. You dont actually expect to encounter it in real life, let alone in the context of critical production infrastructure, any more than you expect to hear Elvish spoken in Congressional debates.

    The Hamiltonian makeover turned the isolationist, small-farmer America of Jeffersons dreams into the epicentre of the technology-driven, planet-hacking project that we call globalisation

But that is the sort of unexpected juxtaposition you routinely encounter when you explore the interior of the American cloud. It is a world as counterintuitive as shopping at Whole Foods is intuitive. Its a space where naked technological realities accumulate behind the scenes in order to enable American life.

The massive datacentres that have recently retreated into the heartland of the US are merely the latest additions to this orchestra of scaled technologies. Together, these systems constitute a single, intricately interconnected entity, woven from a thousand particular technologies that have made the long journey from garage to grid.

I had come to Omaha to explore the American cloud, having succumbed to the Tocquevillean conceit, peculiar to the foreign-born, that one can read America by travelling through it. And so, here I was, discussing continent-spanning infrastructure with hyper-specialised geeks, in a region we romantically associate with the homesteading generalists of historic small-town America. At least in Omaha, such incongruities are readily apparent. In coastal America, where schoolchildren sometimes botch math problems about milk production because they assume a five-day week for cows, the incongruities are masked by the theatre of the shopping experience. But in reality, the cosy coastal world of simulated farmers markets and happy cows bears no resemblance to the actual back end of America.

The American cloud is the product of a national makeover that started in 1791 with Alexander Hamiltons American School of economics — a developmental vision of strong national institutions and protectionist policies designed to shelter a young, industrialising nation from British dominance. Hamiltons vision was diametrically opposed to Thomas Jeffersons competing vision based on small-town, small-scale agrarian economics. Indeed, the story of America is, in many ways, the story of how Hamiltons vision came to prevail over Jeffersons.

By the early 19th century, Hamiltons ideas had crystallised into two complementary doctrines, both known as the American system. The first was senator Henry Clays economic doctrine, based on protectionist tariffs, a national bank, and ongoing internal infrastructure improvements. The second was the technological doctrine of precision manufacturing based on interchangeable parts, which emerged around Springfield and Harpers Ferry national armouries. Together, the two systems would catalyse the emergence of an industrial back end in the countrys heartland, and the establishment of a consumer middle class on the urbanising coasts. But it would take another century, and the development of the internet, for the American cloud to retreat almost entirely from view.

By the 1880s, the two American systems had given rise to a virtuous cycle of accelerating development, with emerging corporations and developing national infrastructure feeding off each other. The result was the first large-scale industrial base a world of ambitious infrastructure projects, giant corporations and arcane political structures. Small farms gave way to transcontinental railroads, giant dams, Standard Oil and US Steel. The most consequential political activity retreated into complex new governance institutions that few ordinary citizens understood, such as the Interstate Commerce Commission, the Federal Reserve, and the War Industries Board. Politics began to acquire its surreal modern focus on broadly comprehensible sideshows.

    Your Kindle is a product, a store, a shopping cart, and a payment system all rolled together

Over the course of two centuries, the Hamiltonian makeover turned the isolationist, small-farmer America of Jeffersons dreams into the epicentre of the technology-driven, planet-hacking project that we call globalisation. The visible signs of the makeover — I call them Hamiltonian cathedrals — are unprepossessing. Viewed from planes or interstate highways, grain silos, power plants, mines, landfills and railroad yards cannot compete visually with big sky and vast prairie. Nevertheless, the Hamiltonian makeover emptied out and transformed the interior of America into a technology-dominated space that still deserves the name heartland. Except that now the heart is an artificial one.

The makeover has been so psychologically disruptive that during the past century, the bulk of Americas cultural resources have been devoted to obscuring the realities of the cloud with simpler, more emotionally satisfying illusions. These constitute a theatre of pre-industrial community life primarily inspired, ironically enough, by Jeffersons small-town visions. This theatre, which forms the backdrop of consumer lifestyles, can be found today inside every Whole Foods, Starbucks and mall in America. I call it the Jeffersonian bazaar.

Structurally then, the American cloud is an assemblage of interconnected Hamiltonian cathedrals, artfully concealed behind a Jeffersonian bazaar. The spatial structure of this American edifice is surprisingly simple a bicoastal surface that is mostly human-habitable bazaar, and a heartland that is mostly highly automated infrastructure cathedrals. In this world, the bazaars are the interiors of cities, forming a user-interface layer over the complex tangle of pipes, cables, dumpsters and loading docks that engineers call the last mile — the part that actually reaches the customer. The cities themselves are cathedrals crafted for human habitation out of steel and concrete. The bazaar is merely a thin fiction lining it. Between the two worlds there is a veil of manufactured normalcy — a studiously maintained aura of the small-town Jeffersonian ideal.

To walk into Whole Foods is to recognise that the Jeffersonian bazaar exists in the interstices of the cloud rather than outside of it. Particular clouds might have insides and outsides — smartphone apps live outside, datacentres live inside; gas stations live outside, oil supertankers live inside — but the cloud as a whole has no meaningful human-inhabited outside. It subsumes bicoastal America rather than being book-ended by it.

Between the first supermarket chains that replaced small-town grocers, and Whole Foods, the special effects have improved but what we inhabit is still recognisably a simulacrum of a Jeffersonian past, not the real thing. To pierce the veil, all you need to do is wander around to the loading docks. The Jeffersonian bazaar is no seamless matrix.

The modern megacity is arguably the most impressive Hamiltonian cathedral of all, not just because of its scale and complexity, but because it manages to fool us into thinking it is merely a scaled-up small town. We are only now beginning to appreciate just how qualitatively different the modern metropolis is from the village, town or small city. For bicoastal Americans, these megacities are the only Hamiltonian cathedrals they ever see up close, during landings and take-offs. Their flight paths over flyover country are far too high for them to see much else.

    The veil of manufactured normalcy exists in the workplace as well, but it is necessarily thinner

While few Americans see much of Hamiltonian America, they do interact with it extensively, through two distinct interfaces. The first interface connects us to the Hamiltonian worlds core abstractions the firm, the market and the law, identified as the central abstractions of modern life by the economist du jour Ronald Coase. The mediating objects of this interface are transactional instruments dollars, votes, contracts, patents, judicial precedents.

The second interface connects us to the Hamiltonian cathedrals themselves. The mediating objects here are the metaphor-laden user-experience widgets of everyday life buttons, shopping carts, light switches, steering wheels, faucets, flush-handles, and trash cans.

Some of our behaviours, such as signing up for direct deposit of paychecks, paying with credit cards, scanning bar codes, and accepting Terms of Service agreements on websites, involve primarily the first interface. Others, such as shopping, using gas stoves, boarding planes or watching television, involve primarily the second. Modern technologies — think check-ins, coupons, in-app purchases, gamified interactions, and star ratings in your favourite app — weave both interfaces into one. Your Kindle is a product, a store, a shopping cart, and a payment system all rolled together.

Like A&Ps George Gillman before them, modern marketers must synthesise a narrative out of thousands of instrumental interactions with distant artificial realities, humanising them for incorporation into the Jeffersonian bazaar. The process is not cheap, which explains why Whole Foods offers a far more compelling bazaar illusion than Costco. Those who want a more seamless illusion must pay more.

The heartlands Hamiltonian cathedrals are bare-metal techno-cultural spaces. By contrast, the Jeffersonian bazaar is, to a first approximation, a purely cultural space where we can remain human in some unreconstructed, romantic sense of the word. Its a space within which we strive to render technology invisible to our appreciative senses, while retaining its instrumental capacities. These two interfaces, the conceptual and the metaphoric, both connect us to, and separate us from, the large-scale systems that provision our lives, allowing us to have our technological cake and eat it, too.

    A touch of humour or irony is sufficient to satisfy even the most refined hipster sensibilities

And so this is how we inhabit the American cloud in the modern age. We finance the Jeffersonian consumption of our evenings and weekends through participation in Hamiltonian production models during our weekdays. Of course, the veil of manufactured normalcy exists in the workplace as well, but it is necessarily thinner. And increasingly, for those information and service workers who live in the coffee shops at the heart of the bazaar, there is no need to go there anyway.

With each passing year, the Jeffersonian pastoral simulation acquires more precision and refinement, if not historical accuracy. In The Theory of the Leisure Class (1899), the economist Thorstein Veblen described the birth of this artifice in the form of the pastoralised estates of the rich. Today, we are observing the completion of the process and its gradual extension to the non-rich. Even among the poor, visceral encounters with back-end realities like pink slime are vanishingly rare. The arms race between technological forces drawing us out of Eden, and the normalisation forces striving to return us to a simulation of it, is entering its end-game.

Of course, we are not entirely unaware of the factory-farming world of food processing firms such as Tyson Fresh Meats and Cargill Foods. We have a dim awareness that our civilisation runs on undocumented Mexican labour, not to mention seven-day weeks for cows and packed feedlots. This is perhaps why the 2013 Superbowl commercial for a Ram pickup truck — a cynical and anachronistic homage to small farmers set to a 1978 speech entitled So God Made a Farmer — offended so many viewers, including many who know little about the reality of factory farming. The narrative was just a little too tasteless to be accepted into the Jeffersonian bazaar.

Yet when presented with just a bit more taste, we swallow such narratives without much questioning. A touch of humour or irony is sufficient to satisfy even the most refined hipster sensibilities. We do not require our marketing narratives to be true. We merely require them to convince us of our own sophistication.

Pop culture plays an important role in the concealment of Hamiltonian realities. Patterns of life in the Jeffersonian bazaar are still derived, via layers of metaphor and symbolism, from pre-industrial realities. But since the illusion is not perfect, we require actors on television to complete the interpretation for us, mediating our relationships to the theatres we inhabit. As a result, American pop culture retains long-term memories of Jeffersonian historical epochs, endlessly repurposing the archetypes of those human-scale eras into contemporary stories. By contrast, Hamiltonian epochs quickly fade from collective memory.

    Periods of technological equilibrium are punctuated by periods of rapid change, creating technological epochs

The brief story of the HBO series Deadwood (2004-06), a western set in the 1870s showcasing a Jeffersonian local polity of gunslingers and prospectors, is now classic television. Tourists still swarm to the real town of Deadwood, South Dakota, making their way from Wild Bill Hickoks grave site to the patch of kitschy Americana that is its modern downtown. But few venture the three miles to Lead, home to the Homestake Mining Company, founded by the villain of the Deadwood TV series, George Hearst. The mine continued to produce gold for shareholders until 2002, but did not merit its own TV show.

I was no exception. Having made my way west from Omaha to Deadwood via North Platte, home to Union Pacifics Bailey Yard — the largest railroad classification yard in the world — I too decided to skip Lead, and headed instead towards Cody, Wyoming. Cody was home to Buffalo Bill Cody, who transformed the tragedy of Wild Bill Hickoks West into the farce of Buffalo Bills Wild West shows of the 1880s and beyond — one of the first major pieces of theatre to be incorporated into the fledgling Jeffersonian bazaar.

A coal mine in Gillette, Wyoming. Photo by Michael Hal/Getty A coal mine in Gillette, Wyoming. Photo by Michael Hal/Getty

Life in the bicoastal Jeffersonian bazaar simply does not prepare you for the sights, sounds and proportions of the Hamiltonian heartland. On the way west to Cody, drawn by the sight of a power plant that loomed menacingly through rain and mist above Interstate 90, I stopped in Gillette, Wyoming, a town of fewer than 29,000 people that bills itself the energy capital of the nation.

Having got lost trying to find my way to the power plant, I ended up at a public viewing area overlooking the Eagle Butte coal mine seven miles out of town. The viewing area features a massive excavator bucket and a tire taller than most humans. A single tire used in mining, I later discovered, can cost $40,000-$70,000. A full set can cost more than an inexpensive home. Contemplating giant tires is one way to appreciate that you have entered a different world.

The Hamiltonian heartland is a land of Brobdingnagian and Lilliputian proportions. It is a land of cryptic and inaudible conversations between radio-frequency ID scanners and passing railroad cars, and records too numerous for the Guinness Book to track. It is also a land of millions upon millions of serial numbers on doors, pieces of equipment, cowlings, pipes, pylons, and an ocean of smaller technological artefacts so vast that, in economics classrooms, they have to be collectively obscured under the label widgets.

If the proportions of the Hamiltonian heartland defy our spatial intuitions, its pace of evolution defies our temporal intuitions. In the time it takes for the heartland to change significantly — 50 to 70 years on average, in the case of major technologies such as the railroad — human-scale heroes typically grow old and die. But change does occur. Periods of technological equilibrium are punctuated by periods of rapid change, creating technological epochs. Each such epoch creates a new layer of visible changes in the Hamiltonian heartland, and corresponding changes in institutions.

These epochs are defined by their abundances and scarcities. Over the past half century, weve been learning how to stop wasting oil and how to start wasting bits, as Alan Kay, one of the inventors of Smalltalk, put it in the 1970s. That knowledge is now transforming the heartland.

From the Smalltalk-speaking computers of Northern Natural Gas to the Union Pacific control room operating the gigantic railroad switchboard that is Bailey Yard, to the missile silos dotting the badlands of South Dakota, our Hamiltonian heartland is being slowly transformed by software and energy-efficiency technologies.

    Weve gone from neighbourhood farms to five-day cows to a world where horsemeat and beef can get accidentally mixed up in the meat cloud

But while large-scale shifts can create drastic changes, they are not clean breaks from the past. Every shift also leaves a good deal unchanged. We can detect major shifts when Hamiltonian cathedrals move or transform at the bare-metal level. The rise of steam, for instance, led to a gradual drift of factories away from water power sources, and the decline of mill towns. It also triggered a simplification in the internal structure of factories, as the belts and pulleys that linked the machines to the mill vanished, to be replaced first by steam engines and pipes, and eventually by wiring and electric motors. It was a shift similar to the one from mainframe to personal computing.

Once a Hamiltonian technological layer settles, its Jeffersonian interfaces also settle. Firms, markets and the law stabilise, the physical last mile hardens, and the little patch of interaction metaphor in your living room becomes familiar. For a while, there is peace. We buy our iPhones, trade somnolent stocks for more exciting ones, acquire new instrumental skills for production and consumption, and swarm into new patterns of life and work.

With each new technological layer, and each evolution of Jeffersonian interfaces, more Hamiltonian realities recede into the cloud. Serial numbers, perhaps the most resistant traces of Hamiltonian realities within the Jeffersonian bazaar, are finally succumbing. We no longer remember any telephone numbers besides our own. IP addresses have given way to domain names. And even those are vanishing into the memories of apps.

This process of retreat is not new. Weve gone from neighbourhood farms to five-day cows to a world where horsemeat and beef can get accidentally mixed up in the meat cloud. But the retreat of numbers completes the veil in a deep way. Software is eating the world, as Netscapes co-founder Marc Andreessen put it in The Wall Street Journal in 2011, allowing us to seal the last reality leaks.

Our metaphors struggle to keep up not just with changing realities, but the growing intricacy of the interfaces to those realities. The metaphor of a passive and translucent veil separating the human and technological worlds has been transformed into the actuality of skeumorphic linings obscuring bare metal. Todays city dwellers rarely realise just how much steel surrounds them wherever they go. The interfaces have thickened and acquired intelligence in proportion to our desire to manipulate Hamiltonian reality.

To the Jeffersonian sensibility, Hamiltonian cathedrals are often little more than infrastructure porn. But to establish a direct, appreciative relationship with these technologies, unmediated by instrumental metaphors and currencies of interaction, you have to walk among them yourself. You have to experience train yards, landfills, radio-frequency ID-tagged seven-day cows and other such backstage oddities in the flesh.

    Thomas Edison thought the gramophone would be primarily used as a dictaphone; instead, it revolutionised music

But it is not sufficient to simply get into a car and drive into flyover country. You need a literate — and literary — perspective, to appreciate what you see, since there are no marketers around to do the work for you.

My own perspective is something like an ironic religion of technological mindfulness, one whose central perceptual act is the projection of a cryptic agency onto the whole that is neither malevolent, nor benevolent, but merely something to be decrypted.

In this act of ritual decryption, it is useful to begin with the assumption that decrypted realities are unlikely to revolve around human concerns. The technological might have originated in the human, but its essence is neither human nor transhuman. It is simply non-human. There is no necessary relationship between what technology does and what humans want. Thomas Edison thought the gramophone would be primarily used as a dictaphone; instead, it revolutionised music. Lee De Forest thought the wireless would take high culture to the masses; instead, it created popular music forms that ended up marginalising classical music. The internet was conceived as a communication system that could survive nuclear war; today we use it to trade kitten pictures.

For me, the ritual pilgrimage into the heartland is an opportunity to reconnect not with the bare-metal cloud in the abstract, but with a thousand particular clouds, each with its own visible motif. Sometimes we can name the motifs that attract our unsupervised attention pylon, container, landfill ventilation pipe, big tire. Other times, we can only pause and remark to ourselves Thats an interesting-looking widget. I wonder what its for?

My pilgrimage into the Hamiltonian heartland ended in Gillette. After contemplating the giant $40,000 tire, I managed to find my way to the power plant that had tempted me off the highway. It turned out to be the Wyodak steam-electric plant, Americas largest air-cooled power plant.

From Gillette, I made my way to Cody, and on through Yellowstone National Park to the home of a wealthy friend in Jackson Hole, perhaps the most complete Jeffersonian bazaar in America — a Whole Foodsian Eden so flawless that only the seriously rich can afford to live there. Someday, technological utopians hope, we might all be able to live somewhere like Jackson Hole. That is, if our Hamiltonian cathedrals dont come crashing down on us first.
Years ago, in my novel Cleaver (2006), I imagined a media man who is used to frantic bustle and talk going in search of silence. He flees to the Alps, looking for a house above the tree line – above, as he begins to think of it, the noise line; a place so high, the air so thin, that he hopes there will be no noise at all. But even in the South Tirol 2,500 metres up, he finds the wind moaning on the rock face, his blood beating in his ears. Then, without any input from his family, his colleagues, the media, his thoughts chatter ever more loudly in his head. As so often happens, the less sound there is outside, the more our own thoughts deafen us.

When we think of silence, because we yearn for it perhaps, or because were scared of it — or both — were forced to recognise that what were talking about is actually a mental state, a question of consciousness. Though the external world no doubt exists, our perception of it is always very much our perception, and tells us as much about ourselves as it does about the world. There are times when a noise out there is truly irritating and has us yearning for peace. Yet there are times when we dont notice it at all. When a book is good, the drone of a distant lawnmower is just not there. When the book is bad but we must read it for an exam, or a review, the sound assaults us ferociously.

If perception of sound depends on our state of mind, then conversely a state of mind can hardly exist without an external world with which it is in relation and that conditions it — either our immediate present environment, or something that happened in the past and that now echoes or goes on happening in our minds. There is never any state of mind that is not in some part, however small, in relation to the sounds around it — the bird singing and a television overheard as I write this now, for example.

Silence, then, is always relative. Our experience of it is more interesting than the acoustic effect itself. And the most interesting kind of silence is that of a mind free of words, free of thoughts, free of language, a mental silence — the state of mind my character Cleaver failed to achieve despite his flight to the mountains. Arguably, when we have a perception of being tormented by noise, a lot of that noise is actually in our heads — the interminable fizz of anxious thoughts or the self-regarding monologue that for much of the time constitutes our consciousness. And its a noise in constant interaction with modern methods of so-called communication the internet, the mobile phone, Google glasses. Our objection to noise in the outer world, very often, is that it makes it harder to focus on the buzz we produce for ourselves in our inner world.

    There is, as it were, a catharsis of exhaustion, exhaustion with the dazzling, disturbing voice of the mind

Yet all of us, at least occasionally, reach the point where the motor of thought feels out of control. Thoughts run away with themselves, go nowhere new, and are nevertheless destructive in their insistent revisiting of where weve been a thousand times before. So much of Modernist literature is about this buzz of consciousness, emphasising its poetic quality. One thinks of James Joyce, or Virginia Woolf. Some, however, understood how exhausting and destructive it could be a character who cant still her thoughts was destroyed into perfect consciousness, writes D H Lawrence in his novel Women in Love (1920). By contrast, a certain genre of late 20th-century literature — from Samuel Beckett through Thomas Bernhard to Sandro Veronesi, David Foster Wallace and many others — is dominated by a voice constantly trying to explain the world, constantly denouncing the scandal of the world, constantly disappointed and frustrated, but also pleased with itself, pleased with its ability to be scandalised, a voice whose ceaseless questioning and criticising has long become a trap, from which consciousness seeks release in various forms of intoxication, or sleep, or suicide. There is, as it were, a catharsis of exhaustion, exhaustion with the dazzling, disturbing voice of the mind.

Such a mental voice is also a source of self-regard. This is the catch that springs the trap. The mind is pleased with the sophistication of its thinking. It wishes the monologue to end, and yet, simultaneously not to end. If it did end, where would identity be? It yearns for silence and fears silence. The two emotions grow stronger together. The more one yearns for silence, the more one fears the loss of identity if the voice should quieten. For example, when a person contemplates a radical change in life — going to live alone in the moors of Galway perhaps, or to a 10-day silent Buddhist retreat — the more he or she fears it, too, fears the moment of change. So our ideas of silence are tied up with questions of self-loathing and self-regard. The end of the monologue is inviting but also frightening, the way children are frightened of going to sleep.

Our desire for silence often has more to do with an inner silence than an outer. Or a combination of the two. Noise provokes our anger, or at least an engagement, and prevents inner silence. But absence of noise exposes us to the loud voice in our heads. This voice is constitutive of what we call self. If we want it to fall silent, arent we yearning for the end of self? For death, perhaps. So talk about silence becomes talk about consciousness, the nature of selfhood, and the modern dilemma in general the desire to invest in the self and the desire for the end of the self.

Of course, we have strategies for getting by. There are soft solutions such as listening to music, or reading. Consciousness is invited to follow someone elses score or storyline. We temporarily hand over the controls to another director. But as soon as we stop reading or listening, the mental noise starts again. We havent resolved anything or learnt anything about ourselves. We havent changed the nature of the discomfort.

More radical, and mortifying perhaps, are solutions involving ritual prayer, rosaries, or mantras. Such an approach feels like a full-scale assault on the self, with an acoustic weapon. Despite, or perhaps because of, my religious childhood, I have never tried this. Ive never desired a mantra. I suspect, as with music, once the mantra is over, the chattering self would bounce back more loquacious and self-righteous than ever.

Or one might try Vipassana — a form of mediation that goes to the heart of this conflict between yearning for silence and fearing it. Without being too specific about why I originally approached Vipassana — lets just say that I had health problems, chronic pain — someone suggested that this discipline might help. I had become aware that though my pains were not, as they say, merely in the mind, my mental state had certainly contributed to the kind of physical tensions that, over many years, had begun to make my life a misery.

The first Vipassana retreat I attended, some five years ago now, was in the mountains north of Milan where I live and work. There seemed no point in going further afield merely to sit on a cushion. In the opening session, I was asked to take a vow of silence for the full 10 days of my stay. So, for all this time, I lived in silence, ate in silence. Above all, I sat for many hours a day, as many as 10, in silence. But there were no chants or mantras to still the mind and get one through. Rather, I was encouraged to substitute, slowly and patiently, my normally talkative consciousness with an intense awareness of breathing and sensation; that is, of the present animal state of our being.

    We use sound and movement to avoid the irksomeness of stasis. You shift from foot to foot, you move from room to room

Its fairly easy to concentrate on the body in motion. If youre running or swimming, its possible to move into a wordless or semi-wordless state that gives the impression of silence for long periods. In fact one of the refreshing, even addictive, things about sport is the feeling that the mind has been given a break from its duty of constantly building up our ego.

But in Vipassana you concentrate on sensation in stillness, sitting down, not necessarily cross-legged, though most people do sit that way. And sitting without changing position, sitting still. As soon as you try to do this, you become aware of a connection between silence and stillness, noise and motion. No sooner are you sitting still than the body is eager to move, or at least to fidget. It grows uncomfortable. In the same way, no sooner is there silence than the mind is eager to talk. In fact we quickly appreciate that sound is movement words move, music moves, through time. We use sound and movement to avoid the irksomeness of stasis. This is particularly true if you are in physical pain. You shift from foot to foot, you move from room to room.

Sitting still, denying yourself physical movement, the minds instinctive reaction is to retreat into its normal buzzing monologue — hoping that focusing the mind elsewhere will relieve physical discomfort. This would normally be the case; normally, if ignored, the body would fidget and shift, to avoid accumulating tension. But on this occasion we are asking it to sit still while we think and, since it cant fidget, it grows more and more tense and uncomfortable. Eventually, this discomfort forces the mind back from its chatter to the body. But finding only discomfort or even pain in the body, it again seeks to escape into language and thought. Back and forth from troubled mind to tormented body, things get worse and worse.

Silence, then, combined with stillness — the two are intimately related — invites us to observe the relationship between consciousness and the body, in movement and moving thought. Much is said when people set off to meditation retreats about the importance of finding themselves. And there is much imagined drama. People expect old traumas to surface, as though in psychoanalysis. In fact, what you actually discover is less personal than you would suppose. You discover how the construct of consciousness and self, something we all share, normally gets through time, to a large extent by ignoring our physical being and existence in the present moment. Some of the early names for meditation in the Pali language of the Buddhist scriptures, far from linking it to religion, referred only to mental exercises.

This form of meditation alters the minds relationship with the body. It invites the meditator to focus attention on all parts of the body equally, without exception, to guide the consciousness through the body and to contem­plate sensation as it ebbs and flows in the flesh, and this without reacting in any way — without aversion to pain, without attachment to pleasure. So we become aware that even when we are still, everything inside us is constantly moving and changing.

Moreover, this activity is not subordinated in the mind to any other. One renounces any objective beyond the contemplation itself. You are not meditating in order to relax, or to overcome pain, or to resolve a health problem, or to achieve inner silence. There is no higher goal but to be present, side by side with the infinitely nuanced flux of sensation in the body. The silence of the mind puts you in touch with the body. Or simply, silence of the mind is awareness of being.

It is hard, at the beginning, to focus, first for minutes at a time, then for hours, on ones breathing. It is hard, at first, to find any sensation at all in many parts of the body when they are still — the temples, the elbows, the calves. Yet once the mind does latch on to sensation, or when sensation responds to the minds patient probing, all at once it becomes easier. Suddenly the body becomes interesting and ones obsessive interest in ones own wordy thoughts begins to dissolve. Language melts away and in the silence all kind of changes occur in the body.

The process is neither that of a single switch being turned, nor of a steady continuum, but of a series of small gains and losses; perhaps a larger step forward, then a small relapse. If one is persistent, undaunted, in ones attempts to concentrate, if one is successful in showing neither aversion to pain nor indulgence in pleasure, then, very slowly, the stillness and silence deepen in an atmosphere of beatitude that is simultaneously and indivisibly both physical and mental. It is as if, as the body is slowly put together and all its component parts unite in an intense present, so the historical self is taken apart and falls away. At no point is it experienced as a loss, but rather as a fullness of existence; something brimful, very ordinary and very beautiful.

The words we constantly use and the narratives we write reinforce a drama of selfhood that we in the West complacently celebrate. There is also much consolation taken in the way in which writing and narrative can transform emotional pain into a form of entertainment, wise and poignant in its vision of our passage through the world, intense and thrilled by its own intensity. Narrative is so often the narrative of misery and of the passage through misery.

What silence and meditation leaves us wondering, after we stand up, unexpectedly refreshed and well-disposed after an hour of stillness and silence, is whether there isnt something deeply perverse in this culture of ours, even in its greatest achievements in narrative and art. So much of what we read, even when it is great entertainment, is deeply unhelpful.
hen I was growing up in New York City, a high point of my calendar was the annual arrival of the Ringling Brothers and Barnum & Bailey Circus — the greatest show on earth. My parents endured the green-haired clowns, sequinned acrobats and festooned elephants as a kind of garish pageantry. For me, though, it was a spectacular interruption of humdrum reality – a world of wonder, in that trite but telling phrase.

Wonder is sometimes said to be a childish emotion, one that we grow out of. But that is surely wrong. As adults, we might experience it when gaping at grand vistas. I was dumbstruck when I first saw a sunset over the Serengeti. We also experience wonder when we discover extraordinary facts. I was enthralled to learn that, when arranged in a line, the neurons in a human brain would stretch the 700 miles from London to Berlin. But why? What purpose could this wide-eyed, slack-jawed feeling serve? Its difficult to determine the biological function of any affect, but whatever it evolved for (and Ill come to that), wonder might be humanitys most important emotion.

First, lets be clear what were talking about. My favourite definition of wonder comes from the 18th-century Scottish moral philosopher Adam Smith, better known for first articulating the tenets of capitalism. He wrote that wonder arises when something quite new and singular is presented… [and] memory cannot, from all its stores, cast up any image that nearly resembles this strange appearance. Smith associated this quality of experience with a distinctive bodily feeling — that staring, and sometimes that rolling of the eyes, that suspension of the breath, and that swelling of the heart.

These bodily symptoms point to three dimensions that might in fact be essential components of wonder. The first is sensory wondrous things engage our senses — we stare and widen our eyes. The second is cognitive such things are perplexing because we cannot rely on past experience to comprehend them. This leads to a suspension of breath, akin to the freezing response that kicks in when we are startled we gasp and say Wow! Finally, wonder has a dimension that can be described as spiritual we look upwards in veneration; hence Smiths invocation of the swelling heart.

English contains many words related to this multifarious emotion. At the mild end of the spectrum, we talk about things being marvellous. More intense episodes might be described as stunning or astonishing. At the extreme, we find experiences of awe and the sublime. These terms seem to refer to the same affect at different levels of intensity, just as anger progresses from mild irritation to violent fury, and sadness ranges from wistfulness to abject despair.

Smiths analysis appears in his History of Astronomy (1795). In that underappreciated work, he proposed that wonder is crucial for science. Astronomers, for instance, are moved by it to investigate the night sky. He might have picked up this idea from the French philosopher René Descartes, who in his Discourse on the Method (1637) described wonder as the emotion that motivates scientists to investigate rainbows and other strange phenomena. In a similar spirit, Socrates said that philosophy begins in wonder that wonder is what leads us to try to understand our world. In our own time, Richard Dawkins has portrayed wonder as a wellspring from which scientific inquiry begins. Animals simply act, seeking satiation, safety and sex. Humans reflect, seeking comprehension.

For a less flattering view, we turn to the 17th-century English philosopher Francis Bacon, the father of the scientific method. He called wonder broken knowledge — a mystified incomprehension that science alone could cure. But this mischaracterises science and wonder alike. Scientists are spurred on by wonder, and they also produce wondrous theories. The paradoxes of quantum theory, the efficiency of the genome these are spectacular. Knowledge does not abolish wonder; indeed, scientific discoveries are often more wondrous than the mysteries they unravel. Without science, we are stuck with the drab world of appearances. With it, we discover endless depths, more astounding that we could have imagined.

In this respect, science shares much with religion. Gods and monsters are wondrous things, recruited to explain lifes unknowns. Also, like science, religion has a striking capacity to make us feel simultaneously insignificant and elevated. Dacher Keltner, professor of psychology at the University of California, Berkeley, has found that awe, an intense form of wonder, makes people feel physically smaller than they are. It is no accident that places of worship often exaggerate these feelings. Temples have grand, looming columns, dazzling stained glass windows, vaulting ceilings, and intricately decorated surfaces. Rituals use song, dance, smell, and elaborate costumes to engage our senses in ways that are bewildering, overwhelming, and transcendent.

Wonder, then, unites science and religion, two of the greatest human institutions. Lets bring in a third. Religion is the first context in which we find art. The Venus of Willendorf appears to be an idol, and animals on the walls of the Chauvet, Altamira and Lascaux caves are thought to have been used in shamanic rites, with participants travelling to imaginative netherworlds in trance-like states under the hypnotic flicker of torchlight. Up through the Renaissance, art primarily appeared in churches. When in the Middle Ages Giotto broke free from the constraints of Gothic painting, he did not produce secular art but a deeply spiritual vision, rendering divine personages more accessible by showing them in fleshy verisimilitude. His Scrovegni Chapel in Padua is like a jewel-box, exploding with figures who breathe, battle, weep, writhe, and rise from the dead to meet their God beneath an ethereal cobalt canopy. It is, in short, a wonder.

When art officially parted company from religion in the 18th century, some links remained. Artists began to be described as creative individuals, whereas the power of creation had formerly been reserved for God alone. With the rise of the signature, artists could obtain cultlike status. A signature showed that this was no longer the product of an anonymous craftsman, and drew attention to the occult powers of the maker, who converted humble oils and pigments into objects of captivating beauty, and brought imaginary worlds to life. The cult of the signature is a recent phenomenon and yet, by promoting reverence for artists, it preserves an old link between beauty and sanctity.

Art museums are a recent invention, too. During the Middle Ages, artworks appeared almost exclusively in religious contexts. After that, they began cropping up in private collections, called cabinets of curiosity (Wunderkammern, in German). These collections intermingled paintings and sculptures with other items deemed marvellous or miraculous animal specimens, fossils, shells, feathers, exotic weapons, decorative books. Art was continuous with science — a human practice whose products could be compared to oddities found in the natural world.

    Art, science and religion are all forms of excess; they transcend the practical ends of daily life

This spirit dominated into the 19th century. The early acquisitions of the British Museum included everything from animal bones to Italian paintings. In a compendious book called The World of Wonders A Record of Things Wonderful in Nature, Science, and Art (1883) we find entries on electric eels, luminous plants, volcanic eruptions, comets, salt mines, the Dead Sea, and dinosaur bones, casually interspersed with entries on Venetian glass, New Zealand wood carvings, and the tomb of Mausolus. The founder of the circus that I used to attend was the showman and charlatan P T Barnum, who took over the American Museum in New York in 1841. There he displayed portraits of famous personages, wax statues, and a scale model of Niagara Falls, at the same time introducing enthralled crowds to the Siamese twins Chang and Eng Bunker, and a little person dubbed General Tom Thumb. The museum was advertised on luminous posters proclaiming the greatest show on Earth — the same show that he would eventually take on the road with his travelling circus. Today, the link between circuses and museums might be hard to fathom, but at the time the connection would have seemed quite natural. As temples of wonder, museums were showcases for oddities a fine portrait, a waxwork tableau and a biological aberration all had their place.

By the end of the century, however, science and art had parted company. Major cities began opening dedicated art museums, places where people could come to view paintings without the distraction of butterfly wings, bearded ladies and deformed animal foetuses in jars. Nowadays, we dont think of museums as houses of curiosity, but they remain places of wonder. They are shrines for art, where we go to be amazed.

Atheist that I am, it took some time for me to realise that I am a spiritual person. I regularly go to museums to stand in mute reverence before the artworks that I admire. Recently, I have been conducting psychological studies with Angelika Seidel, my collaborator at the City University of New York (CUNY), to explore this kind of emotional spell.

We told test subjects to imagine that the Mona Lisa was destroyed in a fire, but that there happened to be a perfect copy that even experts couldnt tell from the original. If they could see just one or the other, would they rather see the ashes of the original Mona Lisa or a perfect duplicate? Eighty per cent of our respondents chose the ashes apparently we disvalue copies and attribute almost magical significance to originals. In another study, we hung reproductions of paintings on a wall and told test subjects either that they were works by famous artists or that they were forgeries. The very same paintings appeared physically larger when attributed to famous artists. We also found that pictures look better and more wondrous when they are placed high on a wall when we have to look up at an artwork, it impresses us more.

In the mid-18th century, the philosopher Edmund Burke hypothesised a connection between aesthetics and fear. In a similar vein, the poet Rainer Maria Rilke proclaimed beauty is nothing but the beginning of terror. To put this association to the test, I, together with Kendall Eskine and Natalie Kacinik, psychologists at CUNY, recently conducted another experiment. First, we scared a subset of our respondents by showing them a startling film in which a zombie jumps out on a seemingly peaceful country road. Then we asked all of our subjects to evaluate some abstract, geometric paintings by El Lissitzky. Those subjects who had been startled found the paintings more stirring, inspiring, interesting, and moving. This link between art and fear relates to the spiritual dimension of wonder. Just as people report fear of God, great art can be overwhelming. It stops us in our tracks and demands worshipful attention.

Bringing these threads together, we can see that science, religion and art are unified in wonder. Each engages our senses, elicits curiosity and instils reverence. Without wonder, it is hard to believe that we would engage in these distinctively human pursuits. Robert Fuller, professor of religious studies at Bradley University in Illinois, contends that it is one of the principal human experiences that lead to belief in an unseen order. In science, that invisible order might include microorganisms and the invisible laws of nature. In religion, we find supernatural powers and divine agents. Artists invent new ways of seeing that give us a fresh perspective on the world we inhabit.

Art, science and religion appear to be uniquely human institutions. This suggests that wonder has a bearing on human uniqueness as such, which in turn raises questions about its origins. Did wonder evolve? Are we the only creatures who experience it?

Descartes claimed that it was innate in human beings; in fact, he called it our most fundamental emotion. The pioneering environmentalist Rachel Carson also posited an inborn sense of wonder, one especially prevalent in children. An alternative possibility is that wonder is a natural by-product of more basic capacities, such as sensory attention, curiosity and respect, the last of which is crucial in social status hierarchies. Extraordinary things trigger all three of these responses at once, evoking the state we call wonder.

Other animals can experience it, too. The primatologist Jane Goodall was observing her chimpanzees in Gombe when she noticed a male chimp gesturing excitedly at a beautiful waterfall. He perched on a nearby rock and gaped at the flowing torrents of water for a good 10 minutes. Goodall and her team saw such responses on several occasions. She concluded that chimps have a sense of wonder, even speculating about a nascent form of spirituality in our simian cousins.

This leaves us with a puzzle. If wonder is found in all human beings and higher primates, why do science, art and religion appear to be recent developments in the history of our species? Anatomically modern humans have been around for 200,000 years, yet the earliest evidence for religious rituals appears about 70,000 years ago, in the Kalahari Desert, and the oldest cave paintings (at El Castillo in Spain) are only 40,000 years old. Science as we know it is much younger than that — perhaps only a few hundred years old. It is also noteworthy that these endeavours are not essential for survival, which means they probably arent direct products of natural selection. Art, science and religion are all forms of excess; they transcend the practical ends of daily life. Perhaps evolution never selected for wonder itself.

And if wonder is shared beyond our own species, why dont we find apes carpooling to church each Sunday? The answer is that the emotion alone is not sufficient. It imbues us with the sense of the extraordinary, but it takes considerable intellectual prowess and creativity to cope with extraordinary things by devising origin myths, conducting experiments and crafting artistic representations. Apes rarely innovate; their wonder is a dead-end street. So it was for our ancestors. For most of our history, humans travelled in small groups in constant search for subsistence, which left little opportunity to devise theories or create artworks. As we gained more control over our environment, resources increased, leading to larger group sizes, more permanent dwellings, leisure time, and a division of labour. Only then could wonder bear its fruit.

Art, science and religion reflect the cultural maturation of our species. Children at the circus are content to ogle at a spectacle. Adults might tire of it, craving wonders that are more profound, fertile, illuminating. For the mature mind, wondrous experience can be used to inspire a painting, a myth or a scientific hypothesis. These things take patience, and an audience equally eager to move beyond the initial state of bewilderment. The late arrival of the most human institutions suggests that our species took some time to reach this stage. We needed to master our environment enough to exceed the basic necessities of survival before we could make use of wonder.

If this story is right, wonder did not evolve for any purpose. It is, rather, a by-product of natural inclinations, and its great human derivatives are not inevitable. But wonder is the accidental impetus behind our greatest achievements. Art, science and religion are inventions for feeding the appetite that wonder excites in us. They also become sources of wonder in their own right, generating epicycles of boundless creativity and enduring inquiry. Each of these institutions allows us to transcend our animality by transporting us to hidden worlds. In harvesting the fruits of wonder, we came into our own as a species.
t starts with the slightly awkward heave — leg up and over the seat, feet locating the stirrups — and the indrawn breath that says Lets go. This is a new discipline for me, this stationary bike, and I make sure to pace myself. I tip from side to side, easily and rhythmically, with a hint of a pulse, my movements mechanical at first, each slight shift of the vista in front of me tied to the downstroke of my foot on the pedal. After a while it becomes mildly hypnotic, not that I recognise this, though at some point I do register that time has blurred, that two or more minutes have clicked off on the digital counter without my noticing — Ive been too caught up in whatever is piping through the wire in my ear, or gotten completely fixated on something Im looking at through one or the other of the two windows. And what do I see out there? Not much. Everything.

Looking is oddly different on the stationary bike. Before I sat on this machine, before the business with the hip, I walked. All the time, miles every day, and it was like I had my looking with me on a leash. That was why I walked, a big part of it anyway. I loved the feeling of the moving eye. The neighbourhood streets were mostly always the same, so I used to pretend my gaze was a lens fixed on a rolling cart, a camera dolly. I would try to walk as evenly as I could so that I could film everything I was passing. And this, for some reason, allowed me to see it differently, put things into a new perspective. Its similar to that other game I like to play. Make a box shape with both hands using thumb and index fingers. Look through, click. There in the little box — or the walking Steadicam — is what you normally see, along with the idea of seeing what you normally see. Which makes it completely different. And this, Im finding, is what happens when I get myself up on the seat and start to pedal.

How to think about this? It has to do with a certain boredom, a basic sameness endured twice a day for 20 minutes. I have the two upstairs windows, one peering down on the street below, a few spindly trees, a utility pole with wires, the visible parts of other peoples houses. The other window faces our neighbours house, into their bedroom window, through which I can see the slightly illuminated rectangle of the far window and, through this, the blurry shape of the next house. A clear line of sight. I get no privileged glimpses of domesticity, though the bedroom is being used by our neighbours grown daughter. Sometimes when I ride I can see her shadowy shape cross through the light. What might she be doing there, I wonder? There is so much time to work up hypotheses when you are spinning pedals round and round, waiting for the time to be up.

The sameness, yes. The sameness of the outer view, and then the sameness of whats right here in front of me. Ive put the bike in my sons bedroom, in front of his desk. Hes away now for college and the room is just as he left it. Thats why Ive put the bike here — to break the spell of that. I plant myself right in the midst. Tick-tock and wobble. The noise of the pedals makes it seem like I am an engine thats running itself, an engine driving these jogs of thinking, these stretches of looking, all this thinking about looking. Open your eyes, I tell myself. Bear down so hard that you forget you are looking, and then let the thing, whatever it is, come at you.

I tilt this way and that. I am thinking of nothing, aware only of what feels like a rim of faint blurring all around the edges of my seeing. I dont know how long I go like this, pedalling, listening as in a dream to the whirring of the spokes, the scratchy hiss of my jeans. But at some point I catch myself studying the tree with its bare branches reaching in toward the window, and the hedge down beside it, crusted with old snow turning purple in the evening light, and then I see how the pavement cracks and buckles just beyond. Things could not be more beautiful. How could they? What would I add or change? What could improve this desk right here in front of me, with its small pile of books, the folded-over sheet of newspaper, and that most curious oblong, that thing that looks for all the world like a dragonfly that has fixed itself there. Each point, I think, is a centre around which a world can be drawn. Its all about attention, I decide. Attention. On the street, in the spot where the pavement dips, a puddle filled with sky. Gray, blue, perfect. How have I been sitting here all this time, looking this way and that, and not seen that glowing patch of changing light? No end to looking, I think, as the room tips lightly from side to side.

    The world might be, as Ludwig Wittgenstein said, everything that is the case — but the case is bigger than it was the number of things available for our regard has increased beyond belief

To pay attention, to attend. To be present, not merely in body — it is an action of the spirit. Attend my words means incline your spirit to my words. Heed them. A sentence is a track along which heeding is drawn. A painting is a visual path that looking follows. A musical composition does the same for listening. Art is a summoning of attention. To create it requires the highest directed focus, as does experiencing it.

The French philosopher Simone Weil said Absolutely unmixed attention is prayer. To attend, etymologically, is to stretch toward, to seek with ones mind and senses. Paying attention is striving toward, thus presupposing a prior wanting, an expectation. We look at a work of art and hope to meet it with our looking; we already have a notion of something to be had, gotten. Reading, at those times when reading matters, we let the words condition an expectation and move toward it.

Side to side, the room lightly, steadily rocks. The aperture narrows down. What catches me here sometimes, provokes me, is the smallest thing, the most neglected thing, one that would escape anyones general regard — mine, too, except that for some strange reason it becomes my mission to consider it, to make it the centre of my looking. Zeroing in on that unlikely shape, that zipper-pull, that dragonfly, I feel the speed and imprecision of most of my looking. Right now there is nothing else. I fix it in the centre of my vision and I direct myself at it. And having identified it, I see it. The gun-metal-coloured tab that widens out from its hinge, with its curved bordering, this — what I can still almost persuade myself is the wing of that insect — is designed to be taken between thumb and forefinger, and then the hinge, connected to the grooved attachment that accepts the two elongated zipper ends, that hinge slides either up or down, pulling the teeth from both sides together so that they mesh. A feat of engineering, but overlooked because so small, so common, another of the innumerable things in the world that are as nothing until, for whatever reason, the need arises. How that changes things! I can imagine looking everywhere, turning the house upside down, because it is the essential thing, the coat must be worn, and Where did I see that thing, I saw it somewhere? For that one moment it is the answer to the question; it is wanted. After which, of course, it falls back out of awareness, into its former near-oblivion. As it has to. What would our lives be if we were forever paying out such regard? We can only distribute attention as we need to, on what we deem to matter most. And what we attend to gives a picture of who we are. One person pays the closest heed to details of dress and domestic furnishing, but gives little thought to animals; another person sees nothing but. And so on.

In previous times, there were fewer things to make a claim on our attentiveness. The world might be, as Ludwig Wittgenstein said, everything that is the case — but the case is bigger than it was the number of things available for our regard has increased beyond belief. No longer are there just the primary material basics, but a whole mad universe of images and signals, figments and streams of information arriving through devices, all of which affect attention itself, altering its reach and intensity.

I put myself through the identical rituals every time, getting on with the same movements, adjusting the earbuds, checking the time — what a tiresome creature I am. I even think this every time. But regularity soothes the soul, and didnt Gustave Flaubert insist that a writer had to be regular and orderly in his life, like a bourgeois, so that he can be violent and original in his work. Yes, I think, pushing into the first rotation — violent, original. Violent. Original. And soon I am spinning along, fine as you please, once again taking up my slow scan of whats in front of me, the two windows, the desk with its rattan-backed chair, before letting myself focus in again on the things on the desk.

But — and here you can envision a non-demonstrative mans non-demonstrative double-take — the dragonfly zipper-pull is not where it was! Its there on the desk, but all askew, at a completely different angle. If this were a film, there would be a bowing of bass strings. I fixate just how did the thing get from point A to point B? If no one else has been here — but then, with a pang of disappointment, I remember. The cleaners, they came yesterday, the desk was obviously dusted — and now I suddenly think of Sherlock Holmes, the stories I read one after another, what it was that so intrigued me. It was precisely this that the solution of a case, any case, without exception, would turn on the most trivial-seeming bit of business, the merest detail. As if Arthur Conan Doyle were testing to see how much could depend on how little. One boot-heel, Holmes discovers, is slightly more worn than its counterpart; a nearly microscopic shred of a certain kind of tobacco is found on the stairway; a document — or a zipper-pull, say — has been moved from one part of the desk to another, indicating, of course, the precise irrefutable sequence of events, the exact trail, and the malefactor. But indicating also — and this is the deeper thing — that nothing, nothing, can be discounted. The action of the world maps itself exactly on its surfaces. If a thing doesnt necessarily matter in itself, it might matter because of what it shows about something else. And the dragonfly zip-pull? What is it showing me, there — here — day after day? Why am I staring at this scrap of metal instead of any one of the dozen other things in my field of vision — from the little Buddha statuettes on the dresser to my left, to the books on the desk to the rattan of the chair and its particular pattern? I cant say for sure. Might it be the shape of the thing, the fact that it looks so much like something its not?

I am getting off-track, even as I sit, immobilised. This detail — incidental, trivial — is just a stepping stone. What really compels, of course, is consciousness, the minds movement through the world. I consider what the American philosopher William James called the blooming, buzzing confusion, the swirl of undisciplined awareness — from the mornings early action of thumb and forefinger squeezing toothpaste onto the brush; to the automatic, incremental movements of measuring water for coffee; to the search through pockets for the keys to the car, and on and on. We are many things, some of them quite noble, but we are also, so very often, mired in the moments particulars, and so are our perceptions. And if consciousness is to be presented credibly, it must to some good extent comprise awareness of minutiae. If you want a more exalted term for this, call it phenomenology. James Joyce, Virginia Woolf, Vladimir Nabokov all planted the flag of their aesthetic here. But whatever the art, whatever the genre, the moves must be strategised. For it happens that attention paid to large subjects is usually taken right up into their thematics. We adjust our focus. Contemplating a canvas of a magnificent panorama, or an arresting portrait, is about engaging the subject — the artist is presenting it to us as important for itself. Staring at a canvas of an apple and a curl of lemon peel inevitably becomes a consideration of perception itself. And so, with the zipper.

Cycling back to my earlier thought, there is a state that precedes attention, a desire or need that makes it possible. Thinking of the ways that I look at art or listen to music, I easily distinguish between the dutiful and the avid. In front of the battle scene, the mythological set-piece, I make myself pay a certain kind of attention. I take in the shapes and colours, obey the visual indicators that guide my eye from one point to another; I know to make myself mindful of the narrative, its thematic intention. I can even experience certain satisfactions, noting and feeling the balance of elements, the accuracy of execution, the expressiveness of certain gestures and features. All of this betokens one kind of attention. But I am not at attention. I do not engage out of my own inclinations so much as obey a series of basic directives, much as when I read a novel that is solidly characterised and plotted but that, for whatever reason, does not have me in its thrall.

Other works — certain paintings, novels, pieces of music — activate a completely different set of responses. When I move into the vicinity of a canvas by the 17th-century Dutch artist Jacob van Ruisdael, for example, even before I have looked, when I have seen only enough in my peripheral vision to suggest that it is one of his, I experience what feels like an inclining toward; I ready myself to attend. I feel myself heightened in a Ruisdael way — which is different than a Vermeer way or a Giacometti way. Its as if I dilate my pupils to absorb the particular colour tones, the marks that are his way of drawing trees, the strategies he uses to create distance in his landscapes. I am looking, moving my eye from point to point, sweeping along the width and breadth of the surface, but what I am attending to is more general, deeper, and hardly requires the verification of intensive looking. The paintings I love induce reverie. With Ruisdael, its easy I draw the landscape fully around me. I suck it into myself, so that I might absent myself from whatever daylight spot I occupy in whatever gallery or museum. I am tantalised by its tones, the strokes of execution, but also by its profound pastness. Not its particular century or period, simply that it is a version of a bygone world.

    There is a big difference between our attempting to pay attention to something and having our attention captured — arrested — by something. That capture is what interests me

Here attention meets distraction or, better yet, daydreaming. They are not the same thing. One is the special curse of our age — the self diluted and thinned to a blur by all the vying signals — while the other hearkens back to childhood, seems the very emblem of the souls freedom. Distraction is a shearing away from focus, a lowering of intensity, whereas daydreaming — the word itself conveys immersed intensity. Associational, intransitive the attending mind is bathed in duration. We have no sense of the clock-face; we are fully absorbed by our thoughts, images and scenarios. Daydreaming is closer to our experience of art.

Absolutely unmixed attention is prayer. I keep coming back to this — it chafes. The more so as I dont think of myself as a believer, even as I grant that being is a mystery beyond all reason. The word prayer — I had to look it up — has a Proto-Indo-European origin. It is a fervent plea to God; it is an expression of helplessness, a putting of oneself before a superior force; it is an expression of thanks, of gratitude, to God or an object of worship. However the action is defined, it involves a wanting or needing. Modifying Weil, I would say that attention is not a neutral focus of awareness on some object or event, but is rather an absence looking to be addressed — it is, in most basic terms, a question looking for an answer. There is a big difference between our attempting to pay attention to something and having our attention captured — arrested — by something. That capture is what interests me.

Side to side, I am making my motionless way through space, listening to music, putting myself into a rhythmic trance of a sort, and I am taking in whatever is in front of me, registering the house opposite, the trees, the street, my sons desk with its pile of books, looking yet again at the one-time dragonfly, the zipper pull. Even with the mystery solved, Ive stayed attuned. Ive been given a metaphysical nudge I have by way of my dissociation become aware of the thingness of the thing I am looking at. When it was stripped of familiar context — a dragonfly that couldnt be — it was, how to put it, nakedly present to me. Something of that estrangement still persists. And it infects me, for as I look up from the desk, yet again taking in the windows, trees, books, they all seem different, hanging in a clearer air — not held together by me as parts of some picture or story, but separate existing things that I am next to. And I feel then — before they fall back into the familiar — that I could just keep looking and looking.

Marcel Proust wrote somewhere that love begins with looking, and the idea is suggestive. But if thats the case, the reverse might also be that true looking begins with love. Theres the quote that I used to repeat like a mantra to writing students, from Flaubert Anything becomes interesting if you look at it long enough. Again, the distinctions, the questions of priority. Is it that the looked-at thing becomes interesting, or that its intrinsic interest gradually emerges? Is the power in the negotiable thing or in the act of looking? If the latter, then the things of the world are already layered with significance, and looking is merely the action that discloses.

The digital counter, marking time, marking distance, clicks off imperturbably, the one number going up as the other drops. I focus in, make some imprecise and speculative calculations, but soon enough I turn away, and — again — confront the room and windows and street and trees, everything fitted back into the old frame, the picture swinging lightly from side to side, the push of my breathing, the numbers just a minuscule eddy in the corner of my vision, and that soon displaced by something else, a new perturbation there — as if the sheerest wisp of a cloud had just blocked the sun, but coming from the window opposite. A shape, for an instant cutting off the light from the rooms back window. Eleanor, of course. Moving from one side of her room to the other, Ive seen it a hundred times, but this once, who knows why, I suddenly get the view reversed. She looks up and notices me here. She pauses. I consider the optics, the relative positions of our separate windows vis-à-vis the days light, guessing whether she can see him quite clearly her hulking neighbour, the man in his black T-shirt and jeans, sitting with his hands laced behind his back, tipping slightly from side to side.

To be seen, to know or imagine ourselves the object of anothers attention — how that feels depends on so many things. In part on the nature of that attention — whether it is neutral, the waitress coming over to the table and smiling pleasantly with her pad in her hand; or irritated, as when we are blocking the intersection with our car and drivers on all sides start hitting their horns.

But really the perspective, the vantage point of another, is so unnatural, so hard to hold. How readily it flips back, becomes again the I looking at the other, who might or might not know she is visible. And of course Im always checking. Every time I get on my bike, usually right as Im getting settled, fixing my earbuds, finding my pace, I take a glance into the window opposite, to see. This is not about voyeurism — though I wont pretend that Im above staring at some person who is unaware of being stared at. There is nothing more interesting than beholding the other — pretty much any other — in his or her native habitat of assumed privacy. But this is not like that. Im only here in daylight hours, and though I am sometimes aware of the blurry shape that I know to be Eleanor, or maybe sometimes her mother, I never see anything distinct. But the awareness does make a difference. Even if the person is facing away, is known to me only as a smudge moving though the faint light — I still feel different than I do if there is no one in the room.

    I was lying in bed just before dawn, awake, as so often happens now — suddenly alert with the sensation of This is it — this is my life! which usually arrives and then just vanishes, but I lay there, eyes closed, and held it

I get an image in my mind. I remember being very young and being in a big European city — old streets, old buildings — with my parents, and thinking, with a childs special pang, that if I lived in this place, here, on this street, in that great brown building, I would never again feel alone. Here I would always know myself safe, always just a few feet from other human beings. And I can still get the same feeling in certain cities, or in certain parts of cities I know. I think How could anyone living here on Commonwealth Avenue ever feel truly alone? I ask it even though I know that there can sometimes be no feeling lonelier than being in a room in a crowded hotel, hearing the muffled sounds of others on all sides.

Eleanors blurry outline — there is nothing joining us, she is very likely unaware of me on the other side of two sets of windows, but the wisp of her outline affects me. I sometimes make it my focus, first just idly wondering what it is she is doing there — if she is in her window seat, what she could be reading with such absorption, assuming that she is reading, but then considering the situation more broadly why she is living at home now, how does she fill her days (no sign of a day job) — but then, more abstractly, more existentially, who is she, what kinds of thing preoccupy this young woman whom I have watched from the time she was a baby just home from the hospital? I realise I know nothing at all about her. Nothing.

I was not on my stationary bike when it came to me what Ive really been wanting to say, though came to me makes it sound like Ive arrived at this — this thought or recognition — for the first time, which would not be at all true. Rather, it might be the fundamental live-with-every-day understanding of my middle age. But I know that there are insights so fundamental, so close to our core, that we walk in their vicinity seeing everything but. Not that we dont at some level know — of course we do — but we still get a feeling of real surprise when we catch them again, and affirm to ourselves again This time I wont forget.

I mean attention in the larger — I want to say ultimate — sense. Attention paid to the life, to the fact of the life, to events and people, their enormous mattering — all the things that could not be more obvious when were brought awake, but that really do get slurred away by distraction, sometimes for long periods, so that when the feeling does come again, it seems like something that needs to be marked, sewn à la Blaise Pascal right into the lining of your coat — where you will always see it and remember.

I was not on the bike when the recognition came this most recent time, though recognitions often come during these trances, when the mind is so susceptible. I was lying in bed just before dawn, awake, as so often happens now — suddenly alert with the sensation of This is it — this is my life! which usually arrives and then just vanishes, but I lay there, eyes closed, and held it. And I knew right then that I could turn my mind to any part of my life and bring it alive. Anything the water fountain at my first school, the feeling of walking with my friend in the pine woods near my house, bouncing up and down at the end of the diving board at Walnut Lake, waking in a tent on hard ground in a dew-soaked sleeping bag, knowing the weight of my newborn son when I held him up over my head. I could point my mind to anything in my life and have it — savour it there in the dark, even as I was telling myself that this must not be forgotten, that it absolutely has to be attended to, that my life will make sense only when every one of these things is known for what it was, or is. I think back on it now, holding myself straight, in purposeful motion, but not moving at all, staring in front of me as the world tips lightly from side to side.

